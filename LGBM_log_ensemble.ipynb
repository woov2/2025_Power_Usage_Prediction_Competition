{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65d7cb9-40e9-49cc-8e97-4b78a8bfe7fe",
   "metadata": {},
   "source": [
    "## Library version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civilian-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- Python & library version --------------------------\n",
      "Python version: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas version: 1.5.2\n",
      "numpy version: 1.21.6\n",
      "matplotlib version: 3.5.2\n",
      "tqdm version: 4.65.2\n",
      "sktime version: 0.20.1\n",
      "lightgbm version: 3.3.5\n",
      "seaborn version: 0.11.2\n",
      "scikit-learn version: 1.1.3\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sktime\n",
    "import tqdm as tq\n",
    "import lightgbm as lgb\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "print(\"-------------------------- Python & library version --------------------------\")\n",
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"numpy version: {}\".format(np.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"tqdm version: {}\".format(tq.__version__))\n",
    "print(\"sktime version: {}\".format(sktime.__version__))\n",
    "print(\"lightgbm version: {}\".format(lgb.__version__))\n",
    "print(\"seaborn version: {}\".format(sns.__version__))\n",
    "print(\"scikit-learn version: {}\".format(skl.__version__))\n",
    "print(\"------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156dcef-65d2-46a5-8bf6-1ddaa9f00f84",
   "metadata": {},
   "source": [
    "## 0. load the libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b5e25e-f9a1-4980-ac53-6cdb73b3d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sklearn.model_selection import StratifiedKFold , KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_columns', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b09e2-4ff2-426a-a11d-7512ef73265b",
   "metadata": {},
   "source": [
    "## 1. preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a621b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "info = pd.read_csv('./data/building_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c4c554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치가 존재하지 않습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'완료'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치가 존재하지 않습니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'완료'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#결측치 확인(train)\n",
    "def check_missing_col(dataframe):\n",
    "    missing_col = []\n",
    "    counted_missing_col = 0\n",
    "    for i, col in enumerate(dataframe.columns):\n",
    "        missing_values = sum(dataframe[col].isna())\n",
    "        is_missing = True if missing_values >= 1 else False\n",
    "        if is_missing:\n",
    "            counted_missing_col += 1\n",
    "            print(f'결측치가 있는 컬럼은: {col}입니다')\n",
    "            print(f'해당 컬럼에 총 {missing_values}개의 결측치가 존재합니다.')\n",
    "#            missing_col.append([col, dataframe[col].dtype])\n",
    "    if counted_missing_col == 0:\n",
    "        print('결측치가 존재하지 않습니다')\n",
    "    return \"완료\"\n",
    "\n",
    "display(check_missing_col(train))\n",
    "display(check_missing_col(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b88c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 215.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#전력소비량 이상치 처리(건물번호를 기준)\n",
    "for num in tqdm(range(train['건물번호'].nunique())):\n",
    "    train.loc[train['건물번호'] == num+1, '전력소비량(kWh)'] = train.loc[train['건물번호'] == num+1, '전력소비량(kWh)'].clip(train.loc[train['건물번호'] == num+1, '전력소비량(kWh)'].quantile(.01), train.loc[train['건물번호'] == num+1, '전력소비량(kWh)'].quantile(.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676516c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#info 메타데이터 병합\n",
    "train = pd.merge(train, info[['건물번호','건물유형','연면적(m2)','냉방면적(m2)']], how = 'left', on ='건물번호')\n",
    "test = pd.merge(test, info[['건물번호','건물유형','연면적(m2)','냉방면적(m2)']], how = 'left', on ='건물번호')\n",
    "\n",
    "str_col = ['건물유형']\n",
    "    \n",
    "for i in str_col:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(train[i])\n",
    "    train[i] = le.transform(train[i])\n",
    "\n",
    "    for label in np.unique(test[i]):\n",
    "        if label not in le.classes_:\n",
    "            le.classes_ = np.append(le.classes_, label)\n",
    "    test[i] = le.transform(test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57429436",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 변수들을 영문명으로 변경\n",
    "cols = ['num_date_time', 'build_num', 'date_time', 'temp', 'prec', 'wind', 'hum', 'isolation', 'sunshine', 'power', 'info', 'area', 'cool_area']\n",
    "train.columns = cols\n",
    "\n",
    "def summer_cos(date):\n",
    "    start_date = datetime.strptime(\"2024-06-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2024-09-14 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    period = (end_date - start_date).total_seconds()\n",
    "\n",
    "    return math.cos(2 * math.pi * (date - start_date).total_seconds() / period)\n",
    "\n",
    "def summer_sin(date):\n",
    "    start_date = datetime.strptime(\"2024-06-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date = datetime.strptime(\"2024-09-14 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    period = (end_date - start_date).total_seconds()\n",
    "\n",
    "    return math.sin(2 * math.pi * (date - start_date).total_seconds() / period)\n",
    "\n",
    "# 시간 관련 변수들 생성\n",
    "date = pd.to_datetime(train.date_time)\n",
    "train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "train['date'] = date.dt.date\n",
    "train['hour'] = date.dt.hour\n",
    "train['day'] = date.dt.day\n",
    "train['weekday'] = date.dt.weekday\n",
    "train['month'] = date.dt.month\n",
    "train['week'] = date.dt.isocalendar().week.astype(np.int32)\n",
    "\n",
    "# 이상치 처리\n",
    "train.loc[13238:13826, 'power'] += 3500\n",
    "train.loc[19161:20343, 'power'] -= 4000\n",
    "train = train.drop(index=range(114240, 114408))\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4330f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def copy_pattern_by_days(\n",
    "    df,\n",
    "    build_num,\n",
    "    target_start,\n",
    "    target_end,\n",
    "    offset_days,          # 예: -7(이전 주), +7(다음 주), +3(3일 뒤) 등\n",
    "    col='power',\n",
    "    dt_col='date_time',\n",
    "    inplace=False\n",
    "):\n",
    "    \"\"\"\n",
    "    [target_start ~ target_end] 구간의 값을\n",
    "    (offset_days 만큼 이동한 구간)의 패턴으로 덮어쓰기.\n",
    "    \"\"\"\n",
    "    _df = df if inplace else df.copy()\n",
    "\n",
    "    ts, te = pd.to_datetime(target_start), pd.to_datetime(target_end)\n",
    "    ss, se = ts + pd.Timedelta(days=offset_days), te + pd.Timedelta(days=offset_days)\n",
    "\n",
    "    m_src = (_df['build_num'] == build_num) & (_df[dt_col] >= ss) & (_df[dt_col] <= se)\n",
    "    m_tgt = (_df['build_num'] == build_num) & (_df[dt_col] >= ts) & (_df[dt_col] <= te)\n",
    "\n",
    "    src_vals = _df.loc[m_src].sort_values(dt_col)[col].values\n",
    "    tgt_idx  = _df.loc[m_tgt].sort_values(dt_col).index\n",
    "\n",
    "    if len(src_vals) == 0 or len(tgt_idx) == 0:\n",
    "        return _df  # 소스/타겟이 없으면 그대로 반환\n",
    "\n",
    "    n = min(len(src_vals), len(tgt_idx))\n",
    "    _df.loc[tgt_idx[:n], col] = src_vals[:n]\n",
    "    return _df\n",
    "\n",
    "\n",
    "def batch_copy_patterns_by_days(\n",
    "    df,\n",
    "    jobs,                 # [(build_num, t_start, t_end, offset_days), ...] 또는 dict 리스트\n",
    "    col='power',\n",
    "    dt_col='date_time',\n",
    "    inplace=False,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    여러 건을 한 번에 처리하는 배치 함수.\n",
    "    jobs 원소 형태:\n",
    "      - 튜플: (build_num, target_start, target_end, offset_days)\n",
    "      - 딕셔너리: {\n",
    "            \"build_num\": ...,\n",
    "            \"target_start\": ...,\n",
    "            \"target_end\": ...,\n",
    "            # 아래 중 하나\n",
    "            \"offset_days\": ...,\n",
    "            \"week_offset\": ...  # 있으면 7*week_offset으로 변환\n",
    "        }\n",
    "      ※ offset_days가 있으면 week_offset보다 우선\n",
    "    \"\"\"\n",
    "    def _parse(job):\n",
    "        if isinstance(job, (list, tuple)) and len(job) == 4:\n",
    "            b, ts, te, od = job\n",
    "            return b, ts, te, od\n",
    "        if isinstance(job, dict):\n",
    "            b  = job['build_num']\n",
    "            ts = job['target_start']\n",
    "            te = job['target_end']\n",
    "            if 'offset_days' in job:\n",
    "                od = job['offset_days']\n",
    "            elif 'week_offset' in job:\n",
    "                od = 7 * job['week_offset']\n",
    "            else:\n",
    "                raise ValueError(\"dict job에는 'offset_days' 또는 'week_offset' 중 하나가 필요합니다.\")\n",
    "            return b, ts, te, od\n",
    "        raise ValueError(\"jobs 항목은 (build_num, start, end, offset_days) 튜플 또는 해당 키를 가진 dict여야 합니다.\")\n",
    "\n",
    "    _df = df if inplace else df.copy()\n",
    "\n",
    "    for job in jobs:\n",
    "        b, ts, te, od = _parse(job)\n",
    "        if verbose:\n",
    "            print(f\"[batch] build_num={b}, target=({ts}~{te}), offset_days={od}\")\n",
    "        _df = copy_pattern_by_days(\n",
    "            _df, b, ts, te, od, col=col, dt_col=dt_col, inplace=True\n",
    "        )\n",
    "    return _df\n",
    "\n",
    "\n",
    "jobs = [\n",
    "    (5, \"2024-08-04 00:00\", \"2024-08-04 23:00\", -7),\n",
    "    (6, \"2024-08-15 00:00\", \"2024-08-15 23:00\", -4),\n",
    "    (6, \"2024-08-16 00:00\", \"2024-08-16 23:00\", -7),\n",
    "    (6, \"2024-08-17 00:00\", \"2024-08-17 23:00\", -7),\n",
    "    (6, \"2024-08-18 00:00\", \"2024-08-18 23:00\", -7),\n",
    "    (7, \"2024-07-07 10:00\", \"2024-07-08 11:00\", -7),\n",
    "    (8,  \"2024-07-21 08:00\", \"2024-07-21 11:00\", -7),\n",
    "    (8,  \"2024-08-24 00:00\", \"2024-08-24 23:00\", -7),\n",
    "    (12, \"2024-07-21 00:00\", \"2024-07-21 23:00\", +7),\n",
    "    (12, \"2024-08-24 00:00\", \"2024-08-24 23:00\", -7),\n",
    "    (17, \"2024-06-25 15:00\", \"2024-06-26 09:00\", -7),\n",
    "    (20, \"2024-06-01 00:00\", \"2024-06-01 23:00\", +7),\n",
    "    (25, \"2024-07-04 12:00\", \"2024-07-04 14:00\", +7),\n",
    "    (26, \"2024-06-17 14:00\", \"2024-06-18 11:00\", -7),\n",
    "    (29, \"2024-06-15 22:00\", \"2024-06-15 23:00\", -7),\n",
    "    (29, \"2024-06-27 00:00\", \"2024-06-27 01:00\", -7),\n",
    "    (30, \"2024-08-04 00:00\", \"2024-08-04 23:00\", -1),\n",
    "    (30, \"2024-08-05 00:00\", \"2024-08-05 23:00\", -1),\n",
    "    (30, \"2024-08-07 00:00\", \"2024-08-07 23:00\", -1),\n",
    "    (40, \"2024-07-14 00:00\", \"2024-07-14 01:00\", -1),\n",
    "    (41, \"2024-06-22 01:00\", \"2024-06-22 04:00\", -7),\n",
    "    (41, \"2024-07-17 00:00\", \"2024-07-17 23:00\", -7),\n",
    "    (42, \"2024-07-17 00:00\", \"2024-07-17 23:00\", -1),\n",
    "    (43, \"2024-06-10 17:00\", \"2024-06-10 18:00\", -7),\n",
    "    (43, \"2024-08-12 16:00\", \"2024-08-12 17:00\", -7),\n",
    "    (43, \"2024-07-20 00:00\", \"2024-07-21 23:00\", -7)\n",
    "]\n",
    "\n",
    "train = batch_copy_patterns_by_days(train, jobs, col='power', dt_col='date_time', inplace=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba098a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_interpolate_building_power(df, targets, target_col='power', method='time'):\n",
    "    \"\"\"\n",
    "    여러 건물/시점(단일 or 구간)에 대해 시간 기반 보간을 한 번에 적용하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): 전체 데이터프레임\n",
    "        targets (list of tuples): \n",
    "            [(build_num, start_time), (build_num, start_time, end_time), ...] 형태의 리스트\n",
    "            - end_time이 없으면 단일 시점 처리\n",
    "        target_col (str): 보간할 컬럼명\n",
    "        method (str): pandas.interpolate method\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 보간이 적용된 원본 데이터프레임\n",
    "    \"\"\"\n",
    "    for item in targets:\n",
    "        # 튜플 길이에 따라 단일 시점/구간 처리\n",
    "        if len(item) == 2:\n",
    "            build_num, start_time = item\n",
    "            end_time = None\n",
    "        elif len(item) == 3:\n",
    "            build_num, start_time, end_time = item\n",
    "        else:\n",
    "            raise ValueError(\"targets는 (build_num, start_time) 또는 (build_num, start_time, end_time) 형식이어야 합니다.\")\n",
    "\n",
    "        # 대상 건물 데이터 추출\n",
    "        building = df[df['build_num'] == build_num].sort_values('date_time').copy()\n",
    "\n",
    "        # 결측 처리\n",
    "        if end_time is None:\n",
    "            mask_missing = (building['date_time'] == pd.Timestamp(start_time))\n",
    "        else:\n",
    "            mask_missing = (\n",
    "                (building['date_time'] >= pd.Timestamp(start_time)) &\n",
    "                (building['date_time'] <= pd.Timestamp(end_time))\n",
    "            )\n",
    "        building.loc[mask_missing, target_col] = np.nan\n",
    "\n",
    "        # 시간 기반 보간\n",
    "        building.set_index('date_time', inplace=True)\n",
    "        building[target_col] = building[target_col].interpolate(method=method)\n",
    "        building.reset_index(inplace=True)\n",
    "\n",
    "        # 원본 반영\n",
    "        df.loc[df['build_num'] == build_num, target_col] = building[target_col].values\n",
    "\n",
    "    return df\n",
    "\n",
    "targets = [\n",
    "    (3, '2024-07-17 14:00'),\n",
    "    (7, '2024-08-06 03:00'),\n",
    "    (18, '2024-07-17 14:00'),\n",
    "    (30, '2024-07-13 20:00'),\n",
    "    (30, '2024-07-25 00:00'),\n",
    "    (42, '2024-07-17 14:00'),\n",
    "    (47, '2024-07-17 14:00'),\n",
    "    (55, '2024-07-17 14:00'),\n",
    "    (76, '2024-08-22 21:00'),\n",
    "    (81, '2024-06-27 14:00'),\n",
    "    (81, '2024-07-17 14:00'),\n",
    "    (82, '2024-07-17 14:00'),\n",
    "    (83, '2024-07-17 14:00'), \n",
    "    (5, '2024-08-04 06:00', '2024-08-04 08:00'), \n",
    "    (18, '2024-06-11 17:00', '2024-06-11 18:00'), \n",
    "    (18, '2024-08-08 15:00', '2024-08-08 16:00'), \n",
    "    (28, '2024-07-17 14:00', '2024-07-17 15:00'), \n",
    "    (38, '2024-07-17 14:00', '2024-07-17 15:00'), \n",
    "    (41, '2024-07-17 09:00', '2024-07-17 15:00'), \n",
    "    (60, '2024-07-17 14:00', '2024-07-17 15:00'), \n",
    "    (62, '2024-07-17 13:00', '2024-07-17 15:00'), \n",
    "    (69, '2024-07-17 14:00', '2024-07-17 15:00'),  \n",
    "    (76, '2024-06-20 12:00', '2024-06-20 16:00'),  \n",
    "    (78, '2024-07-17 13:00', '2024-07-17 14:00'),\n",
    "\n",
    "    # (81, '2024-07-25 13:00', '2024-07-25 17:00'), \n",
    "    # (81, '2024-07-26 13:00', '2024-07-26 17:00'), \n",
    "    # (81, '2024-07-29 13:00', '2024-07-29 17:00'), \n",
    "    # (81, '2024-07-30 13:00', '2024-07-30 17:00'), \n",
    "    # (81, '2024-08-01 13:00', '2024-08-01 17:00'), \n",
    "    # (81, '2024-08-02 13:00', '2024-08-02 17:00'), \n",
    "    # (81, '2024-08-05 13:00', '2024-08-05 17:00'), \n",
    "    # (81, '2024-08-06 13:00', '2024-08-06 16:00'), \n",
    "    # (81, '2024-08-07 13:00', '2024-08-07 17:00'), \n",
    "    # (81, '2024-08-09 13:00', '2024-08-09 17:00'), \n",
    "\n",
    "    # (81, '2024-08-12 10:00', '2024-08-12 15:00'), \n",
    "    # (81, '2024-08-13 13:00', '2024-08-13 17:00'), \n",
    "    # (81, '2024-08-14 10:00', '2024-08-14 17:00'), \n",
    "    # (81, '2024-08-16 10:00', '2024-08-16 17:00'), \n",
    "    # (81, '2024-08-19 10:00', '2024-08-19 17:00'), \n",
    "    # (81, '2024-08-23 11:00', '2024-08-23 16:00'), \n",
    "\n",
    "    (89, '2024-07-12 08:00', '2024-07-12 10:00'), \n",
    "    (97, '2024-07-17 13:00', '2024-07-17 15:00'), \n",
    "]\n",
    "\n",
    "train = batch_interpolate_building_power(train, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8362e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_apply_pattern_scaling(df, tasks, target_col='power'):\n",
    "    \"\"\"\n",
    "    여러 패턴 복사 작업을 한 번에 처리.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): 전체 데이터프레임\n",
    "        tasks (list of tuples): \n",
    "            [\n",
    "                (build_num, source_start, source_end, value_start, value_end, target_start, target_end),\n",
    "                ...\n",
    "            ]\n",
    "        target_col (str): 수정할 컬럼명\n",
    "    Returns:\n",
    "        pd.DataFrame: 수정된 데이터프레임\n",
    "    \"\"\"\n",
    "    for build_num, source_start, source_end, value_start_time, value_end_time, target_start, target_end in tasks:\n",
    "        # 1. 원본 패턴 추출\n",
    "        pattern_mask = (\n",
    "            (df['build_num'] == build_num) &\n",
    "            (df['date_time'] >= pd.Timestamp(source_start)) &\n",
    "            (df['date_time'] <= pd.Timestamp(source_end))\n",
    "        )\n",
    "        P_source = df.loc[pattern_mask].sort_values('date_time')[target_col].values\n",
    "        if len(P_source) == 0:\n",
    "            continue  # 패턴 없으면 스킵\n",
    "\n",
    "        # 2. 시작/종료 값\n",
    "        V_start = df.loc[\n",
    "            (df['build_num'] == build_num) & (df['date_time'] == pd.Timestamp(value_start_time)),\n",
    "            target_col\n",
    "        ].values[0]\n",
    "        V_end = df.loc[\n",
    "            (df['build_num'] == build_num) & (df['date_time'] == pd.Timestamp(value_end_time)),\n",
    "            target_col\n",
    "        ].values[0]\n",
    "\n",
    "        # 3. 정규화 및 스케일링\n",
    "        P_min, P_max = P_source.min(), P_source.max()\n",
    "        P_scaled = (P_source - P_min) / (P_max - P_min + 1e-8)\n",
    "        P_target = V_start + (V_end - V_start) * P_scaled\n",
    "\n",
    "        # 4. 대상 구간 인덱스\n",
    "        target_mask = (\n",
    "            (df['build_num'] == build_num) &\n",
    "            (df['date_time'] >= pd.Timestamp(target_start)) &\n",
    "            (df['date_time'] <= pd.Timestamp(target_end))\n",
    "        )\n",
    "        target_indices = df.loc[target_mask].sort_values('date_time').index\n",
    "\n",
    "        # 5. 길이 맞춰 삽입\n",
    "        length = min(len(P_target), len(target_indices))\n",
    "        df.loc[target_indices[:length], target_col] = P_target[:length]\n",
    "\n",
    "    return df\n",
    "\n",
    "tasks = [\n",
    "    (7, '2024-06-30 10:00', '2024-07-01 11:00', '2024-07-07 09:00', '2024-07-08 12:00', '2024-07-07 10:00', '2024-07-08 11:00'),\n",
    "    (7, '2024-07-05 14:00', '2024-07-05 23:00', '2024-07-12 13:00', '2024-07-13 00:00', '2024-07-12 14:00', '2024-07-12 23:00'),\n",
    "    # (17, '2024-06-18 15:00', '2024-06-19 09:00', '2024-06-25 14:00', '2024-06-26 10:00', '2024-06-25 15:00', '2024-06-26 09:00')\n",
    "]\n",
    "\n",
    "train = batch_apply_pattern_scaling(train, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bdec1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_fill_hourly_means(df, tasks, target_col='power'):\n",
    "    \"\"\"\n",
    "    여러 건물/기간/시간대 평균을 다른 날짜로 삽입하는 배치 함수.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): 전체 데이터프레임\n",
    "        tasks (list of tuples): \n",
    "            [\n",
    "                (build_num, source_dates, source_hours, target_date),\n",
    "                ...\n",
    "            ]\n",
    "            - source_dates: ('start_date','end_date') or ['date1','date2',...]\n",
    "            - source_hours: [hour1, hour2, ...]\n",
    "            - target_date: 단일 날짜\n",
    "        target_col (str): 수정할 컬럼명 (기본 'power')\n",
    "    Returns:\n",
    "        pd.DataFrame: 수정된 데이터프레임\n",
    "    \"\"\"\n",
    "    for build_num, source_dates, source_hours, target_date in tasks:\n",
    "        building = df[df['build_num'] == build_num].copy()\n",
    "\n",
    "        # 날짜 마스크 생성\n",
    "        if isinstance(source_dates, (tuple, list)) and len(source_dates) == 2 and not isinstance(source_dates[0], (pd.Timestamp, str)):\n",
    "            # 범위일 경우\n",
    "            start_date, end_date = pd.to_datetime(source_dates[0]).date(), pd.to_datetime(source_dates[1]).date()\n",
    "            mask_range = (\n",
    "                (building['date_time'].dt.date >= start_date) &\n",
    "                (building['date_time'].dt.date <= end_date) &\n",
    "                (building['date_time'].dt.hour.isin(source_hours))\n",
    "            )\n",
    "        else:\n",
    "            # 날짜 리스트일 경우\n",
    "            date_list = [pd.to_datetime(d).date() for d in source_dates]\n",
    "            mask_range = (\n",
    "                (building['date_time'].dt.date.isin(date_list)) &\n",
    "                (building['date_time'].dt.hour.isin(source_hours))\n",
    "            )\n",
    "\n",
    "        # 시간별 평균 계산\n",
    "        hourly_means = (\n",
    "            building[mask_range]\n",
    "            .groupby(building['date_time'].dt.hour)[target_col]\n",
    "            .mean()\n",
    "            .to_dict()\n",
    "        )\n",
    "\n",
    "        # 타겟 날짜에 삽입\n",
    "        for hour, mean_val in hourly_means.items():\n",
    "            mask_fill = (\n",
    "                (df['build_num'] == build_num) &\n",
    "                (df['date_time'].dt.date == pd.to_datetime(target_date).date()) &\n",
    "                (df['date_time'].dt.hour == hour)\n",
    "            )\n",
    "            df.loc[mask_fill, target_col] = mean_val\n",
    "\n",
    "    return df\n",
    "\n",
    "tasks = [\n",
    "    (67, ('2024-06-03', '2024-06-07'), [16, 17, 18], '2024-06-10'),          # 6/3~6/7 오후 4~6시 평균 → 6/10\n",
    "    (67, ('2024-07-29', '2024-07-31'), [15, 16], '2024-08-01'),              # 7/29~7/31 오후 3~4시 평균 → 8/1\n",
    "    (67, ['2024-08-13', '2024-08-14', '2024-08-16'], [16, 17], '2024-08-12'), # 8/13,14,16 오후 4~5시 평균 → 8/12\n",
    "    (80, ('2024-07-01', '2024-07-05'), [11,12,13,14,19,20], '2024-07-08')    # 7/1~7/5 11~14시,19~20시 평균 → 7/8\n",
    "]\n",
    "\n",
    "train = batch_fill_hourly_means(train, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_power_segments(train: pd.DataFrame, segments: list):\n",
    "    \"\"\"\n",
    "    주어진 구간의 power 값을 앞뒤 하루 같은 시간대 min-max 스케일로 보정.\n",
    "    보정된 값은 train['power']에 덮어씌움.\n",
    "\n",
    "    Parameters:\n",
    "        train (pd.DataFrame): 'build_num', 'date_time', 'power' 컬럼 포함 데이터프레임\n",
    "        segments (list): [(build_num, start_datetime, end_datetime), ...] 형식의 튜플 리스트\n",
    "    Returns:\n",
    "        pd.DataFrame: 보정된 train 데이터프레임\n",
    "    \"\"\"\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "    \n",
    "    for build_num, start_str, end_str in segments:\n",
    "        target_start = pd.Timestamp(start_str)\n",
    "        target_end = pd.Timestamp(end_str)\n",
    "\n",
    "        # 대상 건물 데이터\n",
    "        building_data = train[train['build_num'] == build_num].sort_values(by='date_time')\n",
    "        target_mask = (building_data['date_time'] >= target_start) & (building_data['date_time'] <= target_end)\n",
    "        \n",
    "        # 참조 구간: 앞뒤 하루 동일 시간대\n",
    "        ref_mask = (\n",
    "            ((building_data['date_time'] >= target_start - pd.Timedelta(days=1)) & (building_data['date_time'] <= target_end - pd.Timedelta(days=1))) |\n",
    "            ((building_data['date_time'] >= target_start + pd.Timedelta(days=1)) & (building_data['date_time'] <= target_end + pd.Timedelta(days=1)))\n",
    "        )\n",
    "        ref_data = building_data.loc[ref_mask, 'power']\n",
    "        if ref_data.empty:\n",
    "            continue  # 참조 데이터가 없으면 스킵\n",
    "\n",
    "        ref_min, ref_max = ref_data.min(), ref_data.max()\n",
    "        target_data = building_data.loc[target_mask, 'power']\n",
    "        if target_data.empty or target_data.max() == target_data.min():\n",
    "            continue  # 대상 데이터가 없거나 변동이 없으면 스킵\n",
    "\n",
    "        # 스케일 조정\n",
    "        scaled = (target_data - target_data.min()) / (target_data.max() - target_data.min())  # 0~1 정규화\n",
    "        scaled = scaled * (ref_max - ref_min) + ref_min\n",
    "\n",
    "        # train에 덮어쓰기\n",
    "        train.loc[target_mask & (train['build_num'] == build_num), 'power'] = scaled\n",
    "\n",
    "    return train\n",
    "\n",
    "segments = [\n",
    "    (30, '2024-06-20 06:00', '2024-06-20 23:00'),\n",
    "    (30, '2024-07-06 06:00', '2024-07-06 23:00'),\n",
    "]\n",
    "\n",
    "train = scale_power_segments(train, segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418e5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_weekly_pattern(train: pd.DataFrame, build_num: int, start_str: str, end_str: str):\n",
    "    \"\"\"\n",
    "    특정 구간을 앞주+다음주 동일 시각 데이터 평균으로 채우고 train['power']에 덮어씀.\n",
    "    \"\"\"\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "    building_data = train[train['build_num'] == build_num].sort_values(by='date_time')\n",
    "\n",
    "    # 대상 구간\n",
    "    target_start = pd.Timestamp(start_str)\n",
    "    target_end = pd.Timestamp(end_str)\n",
    "    target_mask = (building_data['date_time'] >= target_start) & (building_data['date_time'] <= target_end)\n",
    "    target_range = building_data.loc[target_mask, ['date_time']].copy()\n",
    "    if target_range.empty:\n",
    "        print(f\"⚠️ 대상 구간({start_str}~{end_str}) 데이터 없음\")\n",
    "        return train\n",
    "\n",
    "    # 앞주 & 다음주 동일 시각 데이터 가져오기\n",
    "    week_offset = pd.Timedelta(days=7)\n",
    "    ref1 = building_data.set_index('date_time').loc[target_start - week_offset : target_end - week_offset, ['power']].reset_index()\n",
    "    ref2 = building_data.set_index('date_time').loc[target_start + week_offset : target_end + week_offset, ['power']].reset_index()\n",
    "\n",
    "    # 두 주 패턴 align (길이가 다를 경우 보정)\n",
    "    if len(ref1) != len(target_range):\n",
    "        ref1 = ref1.reindex(range(len(target_range)), method='nearest')\n",
    "    if len(ref2) != len(target_range):\n",
    "        ref2 = ref2.reindex(range(len(target_range)), method='nearest')\n",
    "\n",
    "    # 두 주 평균 패턴 생성\n",
    "    ref_mean = (ref1['power'].values + ref2['power'].values) / 2\n",
    "    target_range['power_filled'] = ref_mean\n",
    "\n",
    "    # 덮어쓰기\n",
    "    for idx, row in target_range.iterrows():\n",
    "        train.loc[\n",
    "            (train['build_num'] == build_num) & (train['date_time'] == row['date_time']),\n",
    "            'power'\n",
    "        ] = row['power_filled']\n",
    "\n",
    "    return train\n",
    "\n",
    "# 7월 20일 02시 ~ 7월 22일 10시, 건물 49\n",
    "train = fill_with_weekly_pattern(\n",
    "    train, \n",
    "    build_num=43, \n",
    "    start_str=\"2024-07-20 02:00\", \n",
    "    end_str=\"2024-07-22 10:00\"\n",
    ")\n",
    "\n",
    "train = fill_with_weekly_pattern(\n",
    "    train, \n",
    "    build_num=53, \n",
    "    start_str=\"2024-06-14 16:00\", \n",
    "    end_str=\"2024-06-17 09:00\"\n",
    ")\n",
    "\n",
    "train = fill_with_weekly_pattern(\n",
    "    train, \n",
    "    build_num=67, \n",
    "    start_str=\"2024-07-27 00:00\", \n",
    "    end_str=\"2024-07-28 00:00\"\n",
    ")\n",
    "\n",
    "train = fill_with_weekly_pattern(\n",
    "    train, \n",
    "    build_num=94, \n",
    "    start_str=\"2024-07-27 00:00\", \n",
    "    end_str=\"2024-07-28 00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f784bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_to_segment(train: pd.DataFrame, build_num: int, start_str: str, end_str: str, add_value: float):\n",
    "    \"\"\"\n",
    "    특정 건물의 지정 구간에 일정 값을 더해 train['power']에 덮어씀.\n",
    "    \"\"\"\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "    target_start = pd.Timestamp(start_str)\n",
    "    target_end = pd.Timestamp(end_str)\n",
    "\n",
    "    mask = (\n",
    "        (train['build_num'] == build_num) &\n",
    "        (train['date_time'] >= target_start) &\n",
    "        (train['date_time'] <= target_end)\n",
    "    )\n",
    "\n",
    "    train.loc[mask, 'power'] = train.loc[mask, 'power'] + add_value\n",
    "    return train\n",
    "\n",
    "train = add_value_to_segment(\n",
    "    train,\n",
    "    build_num=53,\n",
    "    start_str=\"2024-08-18 16:00\",\n",
    "    end_str=\"2024-08-19 07:00\",\n",
    "    add_value=400\n",
    ")\n",
    "\n",
    "train = add_value_to_segment(\n",
    "    train,\n",
    "    build_num=67,\n",
    "    start_str=\"2024-06-01 00:00\",\n",
    "    end_str=\"2024-06-03 09:00\",\n",
    "    add_value=780\n",
    ")\n",
    "\n",
    "# train = add_value_to_segment(\n",
    "#     train,\n",
    "#     build_num=10,\n",
    "#     start_str=\"2024-06-01 00:00\",\n",
    "#     end_str=\"2024-07-04 07:00\",\n",
    "#     add_value=900\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16012ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_with_prev_next_day_avg(train: pd.DataFrame, build_num: int, start_str: str, end_str: str):\n",
    "    \"\"\"\n",
    "    특정 건물의 지정 구간을 하루 전/하루 뒤 동일 시간대의 평균 값으로 채움.\n",
    "    train['power']에 덮어씀.\n",
    "    \"\"\"\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "    building_data = train[train['build_num'] == build_num].sort_values(by='date_time')\n",
    "\n",
    "    # 대상 구간\n",
    "    target_start = pd.Timestamp(start_str)\n",
    "    target_end = pd.Timestamp(end_str)\n",
    "    target_mask = (building_data['date_time'] >= target_start) & (building_data['date_time'] <= target_end)\n",
    "    target_times = building_data.loc[target_mask, 'date_time']\n",
    "    if target_times.empty:\n",
    "        print(f\"⚠️ 대상 구간({start_str}~{end_str}) 데이터 없음\")\n",
    "        return train\n",
    "\n",
    "    # 하루 전/하루 뒤 동일 시간대 구간\n",
    "    prev_day_mask = (building_data['date_time'] >= target_start - pd.Timedelta(days=1)) & (building_data['date_time'] <= target_end - pd.Timedelta(days=1))\n",
    "    next_day_mask = (building_data['date_time'] >= target_start + pd.Timedelta(days=1)) & (building_data['date_time'] <= target_end + pd.Timedelta(days=1))\n",
    "    prev_data = building_data.loc[prev_day_mask, ['date_time', 'power']]\n",
    "    next_data = building_data.loc[next_day_mask, ['date_time', 'power']]\n",
    "\n",
    "    if prev_data.empty and next_data.empty:\n",
    "        print(f\"⚠️ 참조 데이터 없음({start_str}~{end_str})\")\n",
    "        return train\n",
    "\n",
    "    # 평균 패턴 계산 (있으면 합쳐서 평균)\n",
    "    ref_values = []\n",
    "    if not prev_data.empty:\n",
    "        ref_values.append(prev_data['power'].values)\n",
    "    if not next_data.empty:\n",
    "        ref_values.append(next_data['power'].values)\n",
    "    ref_mean = sum(ref_values) / len(ref_values)  # 두 날짜 평균\n",
    "\n",
    "    # 길이가 다르면 맞춰서 채움\n",
    "    ref_mean_series = pd.Series(ref_mean)\n",
    "    ref_mean_series = ref_mean_series.reindex(range(len(target_times)), method='nearest')\n",
    "\n",
    "    # 덮어쓰기\n",
    "    train.loc[\n",
    "        (train['build_num'] == build_num) & (train['date_time'] >= target_start) & (train['date_time'] <= target_end),\n",
    "        'power'\n",
    "    ] = ref_mean_series.values\n",
    "\n",
    "    return train\n",
    "\n",
    "train = fill_with_prev_next_day_avg(\n",
    "    train,\n",
    "    build_num=70,\n",
    "    start_str=\"2024-06-04 09:00\",\n",
    "    end_str=\"2024-06-05 09:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d3ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_power_with_holiday_pattern(train, build_num, date_ranges):\n",
    "    \"\"\"\n",
    "    특정 건물의 지정 날짜 구간 전력 사용량을\n",
    "    전후 holiday 패턴(같은 시간대 평균)으로 대체하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "        train (pd.DataFrame): 전체 데이터프레임\n",
    "        build_num (int): 건물 번호\n",
    "        date_ranges (list of tuples): [(start_date, end_date), ...] 형식의 구간 리스트 (문자열 or Timestamp)\n",
    "    Returns:\n",
    "        pd.DataFrame: power가 덮어씌워진 원본 train DataFrame\n",
    "    \"\"\"\n",
    "    # 데이터 정렬 및 시간 변환\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'])\n",
    "    building_data = train[train['build_num'] == build_num].sort_values(by='date_time').copy()\n",
    "    building_data['hour'] = building_data['date_time'].dt.hour\n",
    "\n",
    "    # holiday 날짜\n",
    "    holiday_dates = building_data[building_data['holiday'] == 1]['date_time']\n",
    "\n",
    "    for start_date, end_date in date_ranges:\n",
    "        target_start = pd.Timestamp(start_date)\n",
    "        target_end = pd.Timestamp(end_date)\n",
    "\n",
    "        # 전후 holiday 추출\n",
    "        prev_holiday = holiday_dates[holiday_dates < target_start].max()\n",
    "        next_holiday = holiday_dates[holiday_dates > target_end].min()\n",
    "        if pd.isna(prev_holiday) or pd.isna(next_holiday):\n",
    "            continue  # holiday 없으면 skip\n",
    "\n",
    "        # 전후 holiday 패턴\n",
    "        prev_pattern = building_data[building_data['date_time'].dt.date == prev_holiday.date()]\n",
    "        next_pattern = building_data[building_data['date_time'].dt.date == next_holiday.date()]\n",
    "        holiday_pattern = (prev_pattern.groupby('hour')['power'].mean() +\n",
    "                           next_pattern.groupby('hour')['power'].mean()) / 2\n",
    "\n",
    "        # 대체\n",
    "        target_mask = (building_data['date_time'] >= target_start) & (building_data['date_time'] <= target_end)\n",
    "        building_data.loc[target_mask, 'power'] = building_data.loc[target_mask].apply(\n",
    "            lambda row: holiday_pattern.loc[row['hour']] if row['hour'] in holiday_pattern.index else row['power'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # train에 반영\n",
    "    train.loc[building_data.index, 'power'] = building_data['power']\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b48de8a-aec9-42d7-8024-082a0c86e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:56<00:00, 1749.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:55<00:00, 1760.94it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:56<00:00, 1752.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:54<00:00, 1772.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:14<00:00, 2735.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:15<00:00, 2715.26it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:46<00:00, 1907.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 203832/203832 [01:48<00:00, 1880.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>build_num</th>\n",
       "      <th>date_time</th>\n",
       "      <th>temp</th>\n",
       "      <th>prec</th>\n",
       "      <th>wind</th>\n",
       "      <th>hum</th>\n",
       "      <th>isolation</th>\n",
       "      <th>sunshine</th>\n",
       "      <th>power</th>\n",
       "      <th>info</th>\n",
       "      <th>area</th>\n",
       "      <th>cool_area</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_F</th>\n",
       "      <th>WC</th>\n",
       "      <th>THI</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>CDH</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>mean_wind</th>\n",
       "      <th>mean_hum</th>\n",
       "      <th>mean_THI</th>\n",
       "      <th>mean_CDH</th>\n",
       "      <th>mean_WC</th>\n",
       "      <th>z_score</th>\n",
       "      <th>temp_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20240601 00</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-01 00:00:00</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5794.80</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64.94</td>\n",
       "      <td>18.605526</td>\n",
       "      <td>52.9016</td>\n",
       "      <td>14.7</td>\n",
       "      <td>-7.7</td>\n",
       "      <td>20.570833</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>63.375</td>\n",
       "      <td>53.788788</td>\n",
       "      <td>-53.0625</td>\n",
       "      <td>21.342341</td>\n",
       "      <td>11.353622</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20240601 01</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-01 01:00:00</td>\n",
       "      <td>18.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5591.85</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64.94</td>\n",
       "      <td>18.569864</td>\n",
       "      <td>52.9016</td>\n",
       "      <td>14.7</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>20.570833</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>63.375</td>\n",
       "      <td>53.788788</td>\n",
       "      <td>-53.0625</td>\n",
       "      <td>21.342341</td>\n",
       "      <td>10.540611</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20240601 02</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-01 02:00:00</td>\n",
       "      <td>18.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5338.17</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64.58</td>\n",
       "      <td>18.367969</td>\n",
       "      <td>51.6000</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-23.3</td>\n",
       "      <td>20.570833</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>63.375</td>\n",
       "      <td>53.788788</td>\n",
       "      <td>-53.0625</td>\n",
       "      <td>21.342341</td>\n",
       "      <td>9.252619</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20240601 03</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-01 03:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4554.42</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64.40</td>\n",
       "      <td>18.249191</td>\n",
       "      <td>51.8809</td>\n",
       "      <td>14.2</td>\n",
       "      <td>-31.3</td>\n",
       "      <td>20.570833</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>63.375</td>\n",
       "      <td>53.788788</td>\n",
       "      <td>-53.0625</td>\n",
       "      <td>21.342341</td>\n",
       "      <td>7.118597</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20240601 04</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-06-01 04:00:00</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3602.25</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>64.04</td>\n",
       "      <td>18.659442</td>\n",
       "      <td>51.5209</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-39.5</td>\n",
       "      <td>20.570833</td>\n",
       "      <td>17.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>2.75</td>\n",
       "      <td>63.375</td>\n",
       "      <td>53.788788</td>\n",
       "      <td>-53.0625</td>\n",
       "      <td>21.342341</td>\n",
       "      <td>7.868436</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_date_time  build_num           date_time  temp  prec  wind   hum  \\\n",
       "0  1_20240601 00          1 2024-06-01 00:00:00  18.3   0.0   2.6  82.0   \n",
       "1  1_20240601 01          1 2024-06-01 01:00:00  18.3   0.0   2.7  82.0   \n",
       "2  1_20240601 02          1 2024-06-01 02:00:00  18.1   0.0   2.6  80.0   \n",
       "3  1_20240601 03          1 2024-06-01 03:00:00  18.0   0.0   2.6  81.0   \n",
       "4  1_20240601 04          1 2024-06-01 04:00:00  17.8   0.0   1.3  81.0   \n",
       "\n",
       "   isolation  sunshine    power  info      area  cool_area  hour  weekday  \\\n",
       "0        0.0       0.0  5794.80     9  82912.71    77586.0     0        5   \n",
       "1        0.0       0.0  5591.85     9  82912.71    77586.0     1        5   \n",
       "2        0.0       0.0  5338.17     9  82912.71    77586.0     2        5   \n",
       "3        0.0       0.0  4554.42     9  82912.71    77586.0     3        5   \n",
       "4        0.0       0.0  3602.25     9  82912.71    77586.0     4        5   \n",
       "\n",
       "   ...  temp_F         WC      THI  dew_point   CDH  mean_temp  min_temp  \\\n",
       "0  ...   64.94  18.605526  52.9016       14.7  -7.7  20.570833      17.6   \n",
       "1  ...   64.94  18.569864  52.9016       14.7 -15.4  20.570833      17.6   \n",
       "2  ...   64.58  18.367969  51.6000       14.1 -23.3  20.570833      17.6   \n",
       "3  ...   64.40  18.249191  51.8809       14.2 -31.3  20.570833      17.6   \n",
       "4  ...   64.04  18.659442  51.5209       14.0 -39.5  20.570833      17.6   \n",
       "\n",
       "   max_temp  mean_wind  mean_hum   mean_THI  mean_CDH    mean_WC    z_score  \\\n",
       "0      24.8       2.75    63.375  53.788788  -53.0625  21.342341  11.353622   \n",
       "1      24.8       2.75    63.375  53.788788  -53.0625  21.342341  10.540611   \n",
       "2      24.8       2.75    63.375  53.788788  -53.0625  21.342341   9.252619   \n",
       "3      24.8       2.75    63.375  53.788788  -53.0625  21.342341   7.118597   \n",
       "4      24.8       2.75    63.375  53.788788  -53.0625  21.342341   7.868436   \n",
       "\n",
       "   temp_diff  \n",
       "0        7.2  \n",
       "1        7.2  \n",
       "2        7.2  \n",
       "3        7.2  \n",
       "4        7.2  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isolation = pd.pivot_table(train, values = 'isolation', index = ['build_num', 'hour', 'month'], aggfunc = np.mean).reset_index()\n",
    "sunshine = pd.pivot_table(train, values = 'sunshine', index = ['build_num', 'hour', 'month'], aggfunc = np.mean).reset_index()\n",
    "\n",
    "# 건물별, 요일별, 시간별 전력소비량 평균\n",
    "power_mean_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'weekday'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['target_mean_1'] = train.progress_apply(lambda x : power_mean_1.loc[(power_mean_1.build_num == x['build_num']) & (power_mean_1.hour == x['hour']) & (power_mean_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "# 건물별, 요일별, 시간별 전력소비량 표준편차\n",
    "power_std_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'weekday'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train['target_std_1'] = train.progress_apply(lambda x : power_std_1.loc[(power_std_1.build_num == x['build_num']) & (power_std_1.hour == x['hour']) & (power_std_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "## 건물별, 요일별, 시간별 전력소비량 최소\n",
    "power_min_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'weekday'], aggfunc = np.min).reset_index()\n",
    "tqdm.pandas()\n",
    "train['target_min_1'] = train.progress_apply(lambda x : power_min_1.loc[(power_min_1.build_num == x['build_num']) & (power_min_1.hour == x['hour']) & (power_min_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "## 건물별, 요일별, 시간별 전력소비량 최대\n",
    "power_max_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'weekday'], aggfunc = np.max).reset_index()\n",
    "tqdm.pandas()\n",
    "train['target_max_1'] = train.progress_apply(lambda x : power_max_1.loc[(power_max_1.build_num == x['build_num']) & (power_max_1.hour == x['hour']) & (power_max_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "# 건물별, 시간별 전력소비량 평균\n",
    "hour_mean_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_mean_1'] = train.progress_apply(lambda x : hour_mean_1.loc[(hour_mean_1.build_num == x['build_num']) & (hour_mean_1.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "# 건물별, 시간별 전력소비량 표준편차\n",
    "hour_std_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train['hour_std_1'] = train.progress_apply(lambda x : hour_std_1.loc[(hour_std_1.build_num == x['build_num']) & (hour_std_1.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "## 공휴일 변수 추가\n",
    "train['holiday'] = train.apply(lambda x : 0 if x['weekday']<5 else 1, axis = 1)\n",
    "train.loc[('20240606'<=train.date_time)&(train.date_time<'20240607'),'holiday'] = 1\n",
    "train.loc[('20240815'<=train.date_time)&(train.date_time<'20240816'),'holiday'] = 1\n",
    "\n",
    "# 규칙 정의 함수\n",
    "def apply_holiday_rules(row):\n",
    "    bn = row['build_num']\n",
    "    wd = row['weekday']\n",
    "    day = row['day']\n",
    "    week = (row['day'] - 1) // 7 + 1  # 몇째 주인지 계산\n",
    "\n",
    "    # 📌 개별 규칙 적용\n",
    "    if bn == 2:   # 상용: 토요일 쉼 → holiday = 1 if 토요일 else 0\n",
    "        return 1 if wd == 5 else 0\n",
    "    elif bn == 7:   # 건물기타: 일요일 쉼\n",
    "        return 1 if wd == 6 else 0\n",
    "    elif bn == 18:  # 백화점: 일요일 쉼\n",
    "        return 1 if wd == 6 else 0\n",
    "#     elif bn == 19:  # 백화점: 둘째주 월요일 쉼\n",
    "#         return 1 if wd == 0 and week == 2 else 0\n",
    "    elif bn == 25:  # 아파트: 토요일에 적게 씀 (평일로 간주)\n",
    "        return 0\n",
    "    elif bn == 26:  # 건물기타: 주말에 더 씀 → 평일로 간주\n",
    "        return 0 if wd in [5, 6] else 1\n",
    "    elif bn == 27:  # 백화점: 둘째, 넷째주 일요일 쉼\n",
    "        return 1 if wd == 6 and week in [2, 4] else 0\n",
    "    elif bn == 29:  # 백화점: 매달 10일 쉼\n",
    "        return 1 if day == 10 else 0\n",
    "    elif bn == 31:  # 아파트: 휴일 없음 → 평일 취급\n",
    "        return 0\n",
    "    elif bn == 32:  # 백화점: 둘째, 넷째주 월요일 쉼\n",
    "        return 1 if wd == 0 and week in [2, 4] else 0\n",
    "    elif bn == 34:  # 백화점: 휴일 없음 → 평일 취급\n",
    "        return 0\n",
    "    elif bn == 35:  # 전화국: 휴일 없음 -> 평일 취급\n",
    "        return 0\n",
    "    elif bn == 36:  # 전화국: 휴일 없음 -> 평일 취급\n",
    "        return 1 if wd in [5, 6] else 0\n",
    "    elif bn == 40:  # 백화점: 둘째, 넷째주 월요일 쉼\n",
    "        return 1 if wd == 6 and week in [2, 4] else 0\n",
    "    elif bn == 41:\n",
    "        return 0\n",
    "#     elif bn == 45:\n",
    "#         return 1 if day == 10 else 0\n",
    "    elif bn == 54:\n",
    "        return 0\n",
    "    elif bn == 57:\n",
    "        return 0\n",
    "    elif bn == 58:\n",
    "        return 0\n",
    "    elif bn == 59:\n",
    "        return 1 if wd == 6 and week in [2, 4] else 0\n",
    "    elif bn == 61:\n",
    "        return 0\n",
    "    elif bn == 63:\n",
    "        return 1 if wd == 6 and week in [2, 4] else 0\n",
    "    elif bn in [97]:  # 토요일쉼\n",
    "        return 1 if wd == 5 else 0\n",
    "    elif bn in [1,4,9,10,11,19,28,30,33,45,65,70,71,73,74,76,77,78,79,82,84,85,88,89,91,92,93,95,96,98,99,100]:\n",
    "        return 0\n",
    "    else:\n",
    "        # 기본 규칙 유지\n",
    "        return row['holiday']\n",
    "\n",
    "# 규칙 적용\n",
    "train['holiday'] = train.apply(apply_holiday_rules, axis=1)\n",
    "\n",
    "single_day_holidays = [\n",
    "    (19, '2024-06-10'),\n",
    "    (19, '2024-07-08'),\n",
    "    (19, '2024-08-19'),\n",
    "    (23, '2024-06-07'),\n",
    "    (23, '2024-08-16'),\n",
    "    (29, '2024-06-23'),\n",
    "    (29, '2024-07-28'),\n",
    "    (45, '2024-06-10'),\n",
    "    (45, '2024-07-08'),\n",
    "    (45, '2024-08-19'),\n",
    "    (49, '2024-08-22'),\n",
    "    (54, '2024-06-17'),\n",
    "    (54, '2024-07-01'),\n",
    "    (54, '2024-08-19'),\n",
    "    (67, '2024-07-26'),\n",
    "    (79, '2024-06-17'),\n",
    "    (79, '2024-07-01'),\n",
    "    (79, '2024-08-19'),\n",
    "    (94, '2024-06-07'),\n",
    "    (94, '2024-08-16'),\n",
    "    (95, '2024-07-08'),\n",
    "    (95, '2024-08-05'),\n",
    "]\n",
    "\n",
    "for build_num, date_str in single_day_holidays:\n",
    "    target_date = pd.to_datetime(date_str).date()  # 날짜만 비교\n",
    "    train.loc[\n",
    "        (train['build_num'] == build_num) & \n",
    "        (train['date_time'].dt.date == target_date),\n",
    "        'holiday'\n",
    "    ] = 1\n",
    "\n",
    "single_day_no_holiday = [\n",
    "    (67, '2024-06-06')\n",
    "]\n",
    "\n",
    "for build_num, date_str in single_day_no_holiday:\n",
    "    target_date = pd.to_datetime(date_str).date()  # 날짜만 비교\n",
    "    train.loc[\n",
    "        (train['build_num'] == build_num) & \n",
    "        (train['date_time'].dt.date == target_date),\n",
    "        'holiday'\n",
    "    ] = 0\n",
    "\n",
    "date_ranges = [\n",
    "    ('2024-07-26 00:00:00', '2024-07-26 23:59:59'),\n",
    "]\n",
    "train = fill_power_with_holiday_pattern(train, build_num=67, date_ranges=date_ranges)\n",
    "    \n",
    "# 건물별, 휴일별, 시간별 전력소비량 평균\n",
    "power_holiday_mean_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'holiday'], aggfunc = np.mean).reset_index()\n",
    "tqdm.pandas()\n",
    "train['holiday_mean_1'] = train.progress_apply(lambda x : power_holiday_mean_1.loc[(power_holiday_mean_1.build_num == x['build_num']) & (power_holiday_mean_1.hour == x['hour']) & (power_holiday_mean_1.holiday == x['holiday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "# 건물별, 휴일별, 시간별 전력소비량 표준편차\n",
    "power_holiday_std_1 = pd.pivot_table(train, values = 'power', index = ['build_num', 'hour', 'holiday'], aggfunc = np.std).reset_index()\n",
    "tqdm.pandas()\n",
    "train['holiday_std_1'] = train.progress_apply(lambda x : power_holiday_std_1.loc[(power_holiday_std_1.build_num == x['build_num']) & (power_holiday_std_1.hour == x['hour']) & (power_holiday_std_1.holiday == x['holiday']) ,'power'].values[0], axis = 1)\n",
    "    \n",
    "## https://dacon.io/competitions/official/235680/codeshare/2366?page=1&dtype=recent\n",
    "train['sin_hour'] = np.sin(2*np.pi*train.hour/24)\n",
    "train['cos_hour'] = np.cos(2*np.pi*train.hour/24)\n",
    "train['sin_date'] = -np.sin(2 * np.pi * (train['month']+train['day']/31)/12)\n",
    "train['cos_date'] = -np.cos(2 * np.pi * (train['month']+train['day']/31)/12)\n",
    "train['sin_month'] = -np.sin(2 * np.pi * train['month']/12)\n",
    "train['cos_month'] = -np.cos(2 * np.pi * train['month']/12)\n",
    "train['sin_weekday'] = -np.sin(2 * np.pi * (train['weekday']+1)/7)\n",
    "train['cos_weekday'] = -np.cos(2 * np.pi * (train['weekday']+1)/7)\n",
    "\n",
    "#summer_sin, cos\n",
    "train['summer_sin'] = train['date_time'].apply(summer_sin)\n",
    "train['summer_cos'] = train['date_time'].apply(summer_cos)\n",
    "\n",
    "## 화씨 온도\n",
    "train['temp_F'] = (train['temp'] * 9/5) + 32 \n",
    "\n",
    "## 체감 온도\n",
    "train['WC']=13.12+0.6215*train['temp']-13.947*train['wind']**0.16+0.486*train['temp']*train['wind']**0.16\n",
    "\n",
    "## 불쾌 지수\n",
    "train['THI'] = 9/5*train['temp'] - 0.55*(1-train['hum']/100)*(9/5*train['hum']-26)+32\n",
    "\n",
    "train['dew_point'] = train['temp'] - (100 - train['hum']) / 5\n",
    "\n",
    "def CDH(xs):\n",
    "    ys = []\n",
    "    for i in range(len(xs)):\n",
    "        if i < 11:\n",
    "            ys.append(np.sum(xs[:(i+1)]-26))\n",
    "        else:\n",
    "            ys.append(np.sum(xs[(i-11):(i+1)]-26))\n",
    "    return np.array(ys)\n",
    "\n",
    "cdhs = np.array([])\n",
    "for num in range(1,101,1):\n",
    "    temp = train[train['build_num'] == num]\n",
    "    cdh = CDH(temp['temp'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "train['CDH'] = cdhs\n",
    "\n",
    "## mean temperature\n",
    "train = train.merge(train.groupby(['build_num','date'])['temp'].mean().reset_index().rename(columns = {'temp':'mean_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## min temperature\n",
    "train = train.merge(train.groupby(['build_num','date'])['temp'].min().reset_index().rename(columns = {'temp':'min_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## max temperature\n",
    "train = train.merge(train.groupby(['build_num','date'])['temp'].max().reset_index().rename(columns = {'temp':'max_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean windspeed\n",
    "train = train.merge(train.groupby(['build_num','date'])['wind'].mean().reset_index().rename(columns = {'wind':'mean_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## min windspeed\n",
    "# train = train.merge(train.groupby(['build_num','date'])['wind'].min().reset_index().rename(columns = {'wind':'min_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## max windspeed\n",
    "# train = train.merge(train.groupby(['build_num','date'])['wind'].max().reset_index().rename(columns = {'wind':'max_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean humidity\n",
    "train = train.merge(train.groupby(['build_num','date'])['hum'].mean().reset_index().rename(columns = {'hum':'mean_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## min humidity\n",
    "# train = train.merge(train.groupby(['build_num','date'])['hum'].min().reset_index().rename(columns = {'hum':'min_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## max humidity\n",
    "# train = train.merge(train.groupby(['build_num','date'])['hum'].max().reset_index().rename(columns = {'hum':'max_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean THI\n",
    "train = train.merge(train.groupby(['build_num','date'])['THI'].mean().reset_index().rename(columns = {'THI':'mean_THI'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean CDH\n",
    "train = train.merge(train.groupby(['build_num','date'])['CDH'].mean().reset_index().rename(columns = {'CDH':'mean_CDH'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean WC\n",
    "train = train.merge(train.groupby(['build_num','date'])['WC'].mean().reset_index().rename(columns = {'WC':'mean_WC'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## z-score\n",
    "train['z_score'] = train['target_mean_1'] / train['target_std_1']\n",
    "\n",
    "## temp_diff\n",
    "train['temp_diff'] = train['max_temp'] - train['min_temp']\n",
    "\n",
    "train.drop(columns=['date','day'], inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "376440d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1845.54it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:08<00:00, 1894.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1741.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1747.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1730.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1736.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:08<00:00, 1902.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:09<00:00, 1817.00it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:06<00:00, 2678.65it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 16800/16800 [00:06<00:00, 2644.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>build_num</th>\n",
       "      <th>temp</th>\n",
       "      <th>prec</th>\n",
       "      <th>wind</th>\n",
       "      <th>hum</th>\n",
       "      <th>info</th>\n",
       "      <th>area</th>\n",
       "      <th>cool_area</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>holiday</th>\n",
       "      <th>isolation</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_F</th>\n",
       "      <th>WC</th>\n",
       "      <th>THI</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>CDH</th>\n",
       "      <th>mean_temp</th>\n",
       "      <th>min_temp</th>\n",
       "      <th>max_temp</th>\n",
       "      <th>mean_wind</th>\n",
       "      <th>mean_hum</th>\n",
       "      <th>mean_THI</th>\n",
       "      <th>mean_CDH</th>\n",
       "      <th>mean_WC</th>\n",
       "      <th>z_score</th>\n",
       "      <th>temp_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20240825 00</td>\n",
       "      <td>1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>79.70</td>\n",
       "      <td>28.580992</td>\n",
       "      <td>66.7200</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.408333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>76.25</td>\n",
       "      <td>69.47355</td>\n",
       "      <td>18.695833</td>\n",
       "      <td>30.722386</td>\n",
       "      <td>7.365460</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20240825 01</td>\n",
       "      <td>1</td>\n",
       "      <td>26.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.98</td>\n",
       "      <td>29.341150</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>22.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.408333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>76.25</td>\n",
       "      <td>69.47355</td>\n",
       "      <td>18.695833</td>\n",
       "      <td>30.722386</td>\n",
       "      <td>5.735023</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20240825 02</td>\n",
       "      <td>1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>83.0</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.62</td>\n",
       "      <td>28.095476</td>\n",
       "      <td>67.0821</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.408333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>76.25</td>\n",
       "      <td>69.47355</td>\n",
       "      <td>18.695833</td>\n",
       "      <td>30.722386</td>\n",
       "      <td>6.212821</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20240825 03</td>\n",
       "      <td>1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.26</td>\n",
       "      <td>27.613364</td>\n",
       "      <td>66.7221</td>\n",
       "      <td>22.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>28.408333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>76.25</td>\n",
       "      <td>69.47355</td>\n",
       "      <td>18.695833</td>\n",
       "      <td>30.722386</td>\n",
       "      <td>5.765610</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20240825 04</td>\n",
       "      <td>1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>9</td>\n",
       "      <td>82912.71</td>\n",
       "      <td>77586.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>77.90</td>\n",
       "      <td>27.414250</td>\n",
       "      <td>67.9824</td>\n",
       "      <td>22.7</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>28.408333</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.6</td>\n",
       "      <td>1.458333</td>\n",
       "      <td>76.25</td>\n",
       "      <td>69.47355</td>\n",
       "      <td>18.695833</td>\n",
       "      <td>30.722386</td>\n",
       "      <td>8.877454</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_date_time  build_num  temp  prec  wind   hum  info      area  \\\n",
       "0  1_20240825 00          1  26.5   0.0   0.7  80.0     9  82912.71   \n",
       "1  1_20240825 01          1  26.1   0.0   0.0  80.0     9  82912.71   \n",
       "2  1_20240825 02          1  25.9   0.0   0.3  83.0     9  82912.71   \n",
       "3  1_20240825 03          1  25.7   0.0   1.1  83.0     9  82912.71   \n",
       "4  1_20240825 04          1  25.5   0.0   1.0  86.0     9  82912.71   \n",
       "\n",
       "   cool_area  hour  weekday  month  week  holiday  isolation  ...  temp_F  \\\n",
       "0    77586.0     0        6      8    34        0        0.0  ...   79.70   \n",
       "1    77586.0     1        6      8    34        0        0.0  ...   78.98   \n",
       "2    77586.0     2        6      8    34        0        0.0  ...   78.62   \n",
       "3    77586.0     3        6      8    34        0        0.0  ...   78.26   \n",
       "4    77586.0     4        6      8    34        0        0.0  ...   77.90   \n",
       "\n",
       "          WC      THI  dew_point  CDH  mean_temp  min_temp  max_temp  \\\n",
       "0  28.580992  66.7200       22.5  0.5  28.408333      25.0      32.6   \n",
       "1  29.341150  66.0000       22.1  0.6  28.408333      25.0      32.6   \n",
       "2  28.095476  67.0821       22.5  0.5  28.408333      25.0      32.6   \n",
       "3  27.613364  66.7221       22.3  0.2  28.408333      25.0      32.6   \n",
       "4  27.414250  67.9824       22.7 -0.3  28.408333      25.0      32.6   \n",
       "\n",
       "   mean_wind  mean_hum  mean_THI   mean_CDH    mean_WC   z_score  temp_diff  \n",
       "0   1.458333     76.25  69.47355  18.695833  30.722386  7.365460        7.6  \n",
       "1   1.458333     76.25  69.47355  18.695833  30.722386  5.735023        7.6  \n",
       "2   1.458333     76.25  69.47355  18.695833  30.722386  6.212821        7.6  \n",
       "3   1.458333     76.25  69.47355  18.695833  30.722386  5.765610        7.6  \n",
       "4   1.458333     76.25  69.47355  18.695833  30.722386  8.877454        7.6  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set과 동일한 전처리 과정\n",
    "test['일시'] = pd.to_datetime(test['일시'])\n",
    "cols = ['num_date_time', 'build_num', 'date_time', 'temp' , 'prec', 'wind', 'hum', 'info', 'area', 'cool_area']\n",
    "test.columns = cols\n",
    "\n",
    "# 시간 관련 변수들 생성\n",
    "date = pd.to_datetime(test.date_time)\n",
    "test['date'] = date.dt.date\n",
    "test['hour'] = date.dt.hour\n",
    "test['day'] = date.dt.day\n",
    "test['weekday'] = date.dt.weekday\n",
    "test['month'] = date.dt.month\n",
    "test['week'] = date.dt.isocalendar().week.astype(np.int32)\n",
    "\n",
    "test['holiday'] = test.apply(lambda x : 0 if x['weekday']<5 else 1, axis = 1)\n",
    "test['holiday'] = test.apply(apply_holiday_rules, axis=1)\n",
    "\n",
    "## 일조\n",
    "tqdm.pandas()\n",
    "test['isolation'] = np.round(test.progress_apply(lambda x : isolation.loc[(isolation.build_num == x['build_num']) & (isolation.hour == x['hour']) & (isolation.month == x['month']) ,'isolation'].values[0], axis = 1), 1)\n",
    "\n",
    "## 일사\n",
    "tqdm.pandas()\n",
    "test['sunshine'] = np.round(test.progress_apply(lambda x : sunshine.loc[(sunshine.build_num == x['build_num']) & (sunshine.hour == x['hour']) & (sunshine.month == x['month']) ,'sunshine'].values[0], axis = 1), 2)\n",
    "\n",
    "# 건물별, 요일별, 시간별 전력소비량 평균\n",
    "tqdm.pandas()\n",
    "test['target_mean_1'] = test.progress_apply(lambda x : power_mean_1.loc[(power_mean_1.build_num == x['build_num']) & (power_mean_1.hour == x['hour']) & (power_mean_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "# 건물별, 요일별, 시간별 전력소비량 표준편차\n",
    "tqdm.pandas()\n",
    "test['target_std_1'] = test.progress_apply(lambda x : power_std_1.loc[(power_std_1.build_num == x['build_num']) & (power_std_1.hour == x['hour']) & (power_std_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['target_min_1'] = test.progress_apply(lambda x : power_min_1.loc[(power_min_1.build_num == x['build_num']) & (power_min_1.hour == x['hour']) & (power_min_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['target_max_1'] = test.progress_apply(lambda x : power_max_1.loc[(power_max_1.build_num == x['build_num']) & (power_max_1.hour == x['hour']) & (power_max_1.weekday == x['weekday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['holiday_mean_1'] = test.progress_apply(lambda x : power_holiday_mean_1.loc[(power_holiday_mean_1.build_num == x['build_num']) & (power_holiday_mean_1.hour == x['hour']) & (power_holiday_mean_1.holiday == x['holiday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['holiday_std_1'] = test.progress_apply(lambda x : power_holiday_std_1.loc[(power_holiday_std_1.build_num == x['build_num']) & (power_holiday_std_1.hour == x['hour']) & (power_holiday_std_1.holiday == x['holiday']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['hour_mean_1'] = test.progress_apply(lambda x : hour_mean_1.loc[(hour_mean_1.build_num == x['build_num']) & (hour_mean_1.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "tqdm.pandas()\n",
    "test['hour_std_1'] = test.progress_apply(lambda x : hour_std_1.loc[(hour_std_1.build_num == x['build_num']) & (hour_std_1.hour == x['hour']) ,'power'].values[0], axis = 1)\n",
    "\n",
    "test['sin_hour'] = np.sin(2*np.pi*test.hour/24)\n",
    "test['cos_hour'] = np.cos(2*np.pi*test.hour/24)\n",
    "test['sin_date'] = -np.sin(2 * np.pi * (test['month']+test['day']/31)/12)\n",
    "test['cos_date'] = -np.cos(2 * np.pi * (test['month']+test['day']/31)/12)\n",
    "test['sin_month'] = -np.sin(2 * np.pi * test['month']/12)\n",
    "test['cos_month'] = -np.cos(2 * np.pi * test['month']/12)\n",
    "test['sin_weekday'] = -np.sin(2 * np.pi * (test['weekday']+1)/7.0)\n",
    "test['cos_weekday'] = -np.cos(2 * np.pi * (test['weekday']+1)/7.0)\n",
    "\n",
    "#summer_sin, cos\n",
    "test['summer_sin'] = test['date_time'].apply(summer_sin)\n",
    "test['summer_cos'] = test['date_time'].apply(summer_cos)\n",
    "\n",
    "## 화씨 온도\n",
    "test['temp_F'] = (test['temp'] * 9/5) + 32 \n",
    "\n",
    "## 체감 온도\n",
    "test['WC']=13.12+0.6215*test['temp']-13.947*test['wind']**0.16+0.486*test['temp']*test['wind']**0.16\n",
    "\n",
    "test['THI'] = 9/5*test['temp'] - 0.55*(1-test['hum']/100)*(9/5*test['hum']-26)+32\n",
    "\n",
    "test['dew_point'] = test['temp'] - (100 - test['hum']) / 5\n",
    "\n",
    "cdhs = np.array([])\n",
    "for num in range(1,101,1):\n",
    "    temp = test[test['build_num'] == num]\n",
    "    cdh = CDH(temp['temp'].values)\n",
    "    cdhs = np.concatenate([cdhs, cdh])\n",
    "test['CDH'] = cdhs\n",
    "\n",
    "## mean temperature\n",
    "test = test.merge(test.groupby(['build_num','date'])['temp'].mean().reset_index().rename(columns = {'temp':'mean_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## min temperature\n",
    "test = test.merge(test.groupby(['build_num','date'])['temp'].min().reset_index().rename(columns = {'temp':'min_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## max temperature\n",
    "test = test.merge(test.groupby(['build_num','date'])['temp'].max().reset_index().rename(columns = {'temp':'max_temp'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean windspeed\n",
    "test = test.merge(test.groupby(['build_num','date'])['wind'].mean().reset_index().rename(columns = {'wind':'mean_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## min windspeed\n",
    "# test = test.merge(test.groupby(['build_num','date'])['wind'].min().reset_index().rename(columns = {'wind':'min_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## max windspeed\n",
    "# test = test.merge(test.groupby(['build_num','date'])['wind'].max().reset_index().rename(columns = {'wind':'max_wind'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean humidity\n",
    "test = test.merge(test.groupby(['build_num','date'])['hum'].mean().reset_index().rename(columns = {'hum':'mean_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## min humidity\n",
    "# test = test.merge(test.groupby(['build_num','date'])['hum'].min().reset_index().rename(columns = {'hum':'min_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "# ## max humidity\n",
    "# test = test.merge(test.groupby(['build_num','date'])['hum'].max().reset_index().rename(columns = {'hum':'max_hum'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean THI\n",
    "test = test.merge(test.groupby(['build_num','date'])['THI'].mean().reset_index().rename(columns = {'THI':'mean_THI'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean CDH\n",
    "test = test.merge(test.groupby(['build_num','date'])['CDH'].mean().reset_index().rename(columns = {'CDH':'mean_CDH'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## mean WC\n",
    "test = test.merge(test.groupby(['build_num','date'])['WC'].mean().reset_index().rename(columns = {'WC':'mean_WC'}), on = ['build_num','date'], how = 'left')\n",
    "\n",
    "## z-score\n",
    "test['z_score'] = test['target_mean_1'] / test['target_std_1']\n",
    "\n",
    "## temp_diff\n",
    "test['temp_diff'] = test['max_temp'] - test['min_temp']\n",
    "\n",
    "test.drop(['date_time','date','day'], axis = 1, inplace = True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa4dc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['hum'] = test['hum'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44019bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(columns=['num_date_time','date_time','power'])\n",
    "y_train = train['power'].values\n",
    "x_test = test.drop(columns=['num_date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82ca71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[x_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f205cbf-fbd7-45c0-9e08-7c3ab18db988",
   "metadata": {},
   "source": [
    "## 2. Model : LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e136bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMAPE(true, pred):\n",
    "    return np.mean((np.abs(true-pred))/(np.abs(true) + np.abs(pred))) * 200\n",
    "\n",
    "def smape(preds, target):\n",
    "    '''\n",
    "    Function to calculate SMAPE\n",
    "    '''\n",
    "    n = len(preds)\n",
    "    masked_arr = ~((preds==0)&(target==0))\n",
    "    preds, target = preds[masked_arr], target[masked_arr]\n",
    "    num = np.abs(preds-target)\n",
    "    denom = np.abs(preds)+np.abs(target)\n",
    "    smape_val = (200*np.sum(num/denom))/n\n",
    "    return smape_val\n",
    "\n",
    "def lgbm_smape(preds, train_data):\n",
    "    '''\n",
    "    Custom Evaluation Function for LGBM\n",
    "    '''\n",
    "    # labels = train_data.get_label()\n",
    "    labels = train_data\n",
    "    smape_val = smape(preds, labels)\n",
    "    return 'SMAPE', smape_val, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9e38128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['build_num','hour','month','weekday','holiday']\n",
    "\n",
    "is_holdout = False\n",
    "iterations = 80000\n",
    "patience = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d14407ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['build_num', 'holiday', 'hour', 'month', 'weekday']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_0's l2: 0.00255485\tvalid_0's SMAPE: 0.426943\n",
      "[6000]\tvalid_0's l2: 0.00230427\tvalid_0's SMAPE: 0.398912\n",
      "[9000]\tvalid_0's l2: 0.00220983\tvalid_0's SMAPE: 0.387095\n",
      "[12000]\tvalid_0's l2: 0.0021662\tvalid_0's SMAPE: 0.380652\n",
      "[15000]\tvalid_0's l2: 0.00214265\tvalid_0's SMAPE: 0.377449\n",
      "[18000]\tvalid_0's l2: 0.00212623\tvalid_0's SMAPE: 0.37511\n",
      "[21000]\tvalid_0's l2: 0.00211517\tvalid_0's SMAPE: 0.373478\n",
      "[24000]\tvalid_0's l2: 0.00210781\tvalid_0's SMAPE: 0.372346\n",
      "[27000]\tvalid_0's l2: 0.00210354\tvalid_0's SMAPE: 0.37165\n",
      "[30000]\tvalid_0's l2: 0.00210069\tvalid_0's SMAPE: 0.371174\n",
      "[33000]\tvalid_0's l2: 0.00209829\tvalid_0's SMAPE: 0.370786\n",
      "[36000]\tvalid_0's l2: 0.00209622\tvalid_0's SMAPE: 0.370491\n",
      "[39000]\tvalid_0's l2: 0.0020951\tvalid_0's SMAPE: 0.370316\n",
      "[42000]\tvalid_0's l2: 0.00209428\tvalid_0's SMAPE: 0.370159\n",
      "[45000]\tvalid_0's l2: 0.00209355\tvalid_0's SMAPE: 0.370038\n",
      "[48000]\tvalid_0's l2: 0.00209307\tvalid_0's SMAPE: 0.369956\n",
      "[51000]\tvalid_0's l2: 0.0020926\tvalid_0's SMAPE: 0.369877\n",
      "[54000]\tvalid_0's l2: 0.00209227\tvalid_0's SMAPE: 0.369817\n",
      "[57000]\tvalid_0's l2: 0.002092\tvalid_0's SMAPE: 0.369774\n",
      "[60000]\tvalid_0's l2: 0.00209177\tvalid_0's SMAPE: 0.369731\n",
      "[63000]\tvalid_0's l2: 0.00209162\tvalid_0's SMAPE: 0.369704\n",
      "1 FOLD SMAPE :  2.6232733597857902\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00279509\tvalid_0's SMAPE: 0.434206\n",
      "[6000]\tvalid_0's l2: 0.00254491\tvalid_0's SMAPE: 0.407605\n",
      "[9000]\tvalid_0's l2: 0.00245057\tvalid_0's SMAPE: 0.396194\n",
      "[12000]\tvalid_0's l2: 0.00240595\tvalid_0's SMAPE: 0.390391\n",
      "[15000]\tvalid_0's l2: 0.00237911\tvalid_0's SMAPE: 0.386631\n",
      "[18000]\tvalid_0's l2: 0.00236269\tvalid_0's SMAPE: 0.384321\n",
      "[21000]\tvalid_0's l2: 0.00235186\tvalid_0's SMAPE: 0.382774\n",
      "[24000]\tvalid_0's l2: 0.00234413\tvalid_0's SMAPE: 0.381675\n",
      "[27000]\tvalid_0's l2: 0.00233991\tvalid_0's SMAPE: 0.380953\n",
      "[30000]\tvalid_0's l2: 0.00233655\tvalid_0's SMAPE: 0.380436\n",
      "[33000]\tvalid_0's l2: 0.0023341\tvalid_0's SMAPE: 0.380035\n",
      "[36000]\tvalid_0's l2: 0.00233242\tvalid_0's SMAPE: 0.379763\n",
      "[39000]\tvalid_0's l2: 0.00233109\tvalid_0's SMAPE: 0.379539\n",
      "[42000]\tvalid_0's l2: 0.00233005\tvalid_0's SMAPE: 0.379382\n",
      "[45000]\tvalid_0's l2: 0.00232927\tvalid_0's SMAPE: 0.37925\n",
      "[48000]\tvalid_0's l2: 0.00232886\tvalid_0's SMAPE: 0.379161\n",
      "[51000]\tvalid_0's l2: 0.00232845\tvalid_0's SMAPE: 0.379092\n",
      "[54000]\tvalid_0's l2: 0.0023281\tvalid_0's SMAPE: 0.379037\n",
      "2 FOLD SMAPE :  2.705233561106751\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00266222\tvalid_0's SMAPE: 0.427671\n",
      "[6000]\tvalid_0's l2: 0.00240152\tvalid_0's SMAPE: 0.400963\n",
      "[9000]\tvalid_0's l2: 0.00231494\tvalid_0's SMAPE: 0.390111\n",
      "[12000]\tvalid_0's l2: 0.00226782\tvalid_0's SMAPE: 0.383923\n",
      "[15000]\tvalid_0's l2: 0.00224071\tvalid_0's SMAPE: 0.380255\n",
      "[18000]\tvalid_0's l2: 0.00222541\tvalid_0's SMAPE: 0.378052\n",
      "[21000]\tvalid_0's l2: 0.00221416\tvalid_0's SMAPE: 0.37653\n",
      "[24000]\tvalid_0's l2: 0.00220676\tvalid_0's SMAPE: 0.375472\n",
      "[27000]\tvalid_0's l2: 0.00220163\tvalid_0's SMAPE: 0.374763\n",
      "[30000]\tvalid_0's l2: 0.00219912\tvalid_0's SMAPE: 0.3744\n",
      "[33000]\tvalid_0's l2: 0.00219678\tvalid_0's SMAPE: 0.374037\n",
      "[36000]\tvalid_0's l2: 0.00219492\tvalid_0's SMAPE: 0.37379\n",
      "[39000]\tvalid_0's l2: 0.00219362\tvalid_0's SMAPE: 0.373584\n",
      "[42000]\tvalid_0's l2: 0.00219264\tvalid_0's SMAPE: 0.373448\n",
      "[45000]\tvalid_0's l2: 0.00219205\tvalid_0's SMAPE: 0.37336\n",
      "[48000]\tvalid_0's l2: 0.00219154\tvalid_0's SMAPE: 0.37328\n",
      "[51000]\tvalid_0's l2: 0.00219109\tvalid_0's SMAPE: 0.373213\n",
      "[54000]\tvalid_0's l2: 0.00219084\tvalid_0's SMAPE: 0.373174\n",
      "[57000]\tvalid_0's l2: 0.0021906\tvalid_0's SMAPE: 0.373138\n",
      "3 FOLD SMAPE :  2.6533514936131217\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00254288\tvalid_0's SMAPE: 0.426765\n",
      "[6000]\tvalid_0's l2: 0.00231437\tvalid_0's SMAPE: 0.400103\n",
      "[9000]\tvalid_0's l2: 0.00223122\tvalid_0's SMAPE: 0.388624\n",
      "[12000]\tvalid_0's l2: 0.00218781\tvalid_0's SMAPE: 0.382547\n",
      "[15000]\tvalid_0's l2: 0.00216559\tvalid_0's SMAPE: 0.379295\n",
      "[18000]\tvalid_0's l2: 0.00215139\tvalid_0's SMAPE: 0.37701\n",
      "[21000]\tvalid_0's l2: 0.0021416\tvalid_0's SMAPE: 0.375445\n",
      "[24000]\tvalid_0's l2: 0.0021355\tvalid_0's SMAPE: 0.374347\n",
      "[27000]\tvalid_0's l2: 0.00213104\tvalid_0's SMAPE: 0.373665\n",
      "[30000]\tvalid_0's l2: 0.00212788\tvalid_0's SMAPE: 0.373123\n",
      "[33000]\tvalid_0's l2: 0.00212587\tvalid_0's SMAPE: 0.372765\n",
      "[36000]\tvalid_0's l2: 0.00212431\tvalid_0's SMAPE: 0.372522\n",
      "[39000]\tvalid_0's l2: 0.00212302\tvalid_0's SMAPE: 0.372301\n",
      "[42000]\tvalid_0's l2: 0.0021222\tvalid_0's SMAPE: 0.372161\n",
      "4 FOLD SMAPE :  2.6429275747726724\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00277849\tvalid_0's SMAPE: 0.434353\n",
      "[6000]\tvalid_0's l2: 0.00253816\tvalid_0's SMAPE: 0.407424\n",
      "[9000]\tvalid_0's l2: 0.00244119\tvalid_0's SMAPE: 0.395137\n",
      "[12000]\tvalid_0's l2: 0.00239701\tvalid_0's SMAPE: 0.389007\n",
      "[15000]\tvalid_0's l2: 0.00237256\tvalid_0's SMAPE: 0.385468\n",
      "[18000]\tvalid_0's l2: 0.0023598\tvalid_0's SMAPE: 0.383274\n",
      "[21000]\tvalid_0's l2: 0.00235145\tvalid_0's SMAPE: 0.381843\n",
      "[24000]\tvalid_0's l2: 0.00234611\tvalid_0's SMAPE: 0.380833\n",
      "[27000]\tvalid_0's l2: 0.00234262\tvalid_0's SMAPE: 0.380121\n",
      "[30000]\tvalid_0's l2: 0.0023395\tvalid_0's SMAPE: 0.379611\n",
      "[33000]\tvalid_0's l2: 0.00233689\tvalid_0's SMAPE: 0.379172\n",
      "[36000]\tvalid_0's l2: 0.00233574\tvalid_0's SMAPE: 0.378903\n",
      "[39000]\tvalid_0's l2: 0.00233467\tvalid_0's SMAPE: 0.378687\n",
      "[42000]\tvalid_0's l2: 0.00233389\tvalid_0's SMAPE: 0.378527\n",
      "[45000]\tvalid_0's l2: 0.0023333\tvalid_0's SMAPE: 0.37839\n",
      "[48000]\tvalid_0's l2: 0.00233292\tvalid_0's SMAPE: 0.37831\n",
      "[51000]\tvalid_0's l2: 0.00233265\tvalid_0's SMAPE: 0.378243\n",
      "[54000]\tvalid_0's l2: 0.00233246\tvalid_0's SMAPE: 0.378196\n",
      "[57000]\tvalid_0's l2: 0.00233216\tvalid_0's SMAPE: 0.378141\n",
      "[60000]\tvalid_0's l2: 0.00233204\tvalid_0's SMAPE: 0.378108\n",
      "5 FOLD SMAPE :  2.683215835912677\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00258405\tvalid_0's SMAPE: 0.427434\n",
      "[6000]\tvalid_0's l2: 0.00235922\tvalid_0's SMAPE: 0.401006\n",
      "[9000]\tvalid_0's l2: 0.00227727\tvalid_0's SMAPE: 0.390496\n",
      "[12000]\tvalid_0's l2: 0.00223791\tvalid_0's SMAPE: 0.385028\n",
      "[15000]\tvalid_0's l2: 0.0022128\tvalid_0's SMAPE: 0.38162\n",
      "[18000]\tvalid_0's l2: 0.00219914\tvalid_0's SMAPE: 0.379589\n",
      "[21000]\tvalid_0's l2: 0.00218942\tvalid_0's SMAPE: 0.378182\n",
      "[24000]\tvalid_0's l2: 0.00218296\tvalid_0's SMAPE: 0.377172\n",
      "[27000]\tvalid_0's l2: 0.00217809\tvalid_0's SMAPE: 0.376455\n",
      "[30000]\tvalid_0's l2: 0.00217526\tvalid_0's SMAPE: 0.375964\n",
      "[33000]\tvalid_0's l2: 0.00217291\tvalid_0's SMAPE: 0.375557\n",
      "[36000]\tvalid_0's l2: 0.00217135\tvalid_0's SMAPE: 0.375266\n",
      "[39000]\tvalid_0's l2: 0.00217009\tvalid_0's SMAPE: 0.375042\n",
      "[42000]\tvalid_0's l2: 0.00216915\tvalid_0's SMAPE: 0.374882\n",
      "[45000]\tvalid_0's l2: 0.00216851\tvalid_0's SMAPE: 0.374763\n",
      "[48000]\tvalid_0's l2: 0.00216794\tvalid_0's SMAPE: 0.374668\n",
      "[51000]\tvalid_0's l2: 0.00216749\tvalid_0's SMAPE: 0.3746\n",
      "[54000]\tvalid_0's l2: 0.00216716\tvalid_0's SMAPE: 0.374541\n",
      "[57000]\tvalid_0's l2: 0.00216692\tvalid_0's SMAPE: 0.37449\n",
      "[60000]\tvalid_0's l2: 0.00216675\tvalid_0's SMAPE: 0.374455\n",
      "[63000]\tvalid_0's l2: 0.00216659\tvalid_0's SMAPE: 0.374425\n",
      "[66000]\tvalid_0's l2: 0.00216644\tvalid_0's SMAPE: 0.374397\n",
      "[69000]\tvalid_0's l2: 0.00216633\tvalid_0's SMAPE: 0.374379\n",
      "[72000]\tvalid_0's l2: 0.00216623\tvalid_0's SMAPE: 0.37436\n",
      "[75000]\tvalid_0's l2: 0.00216615\tvalid_0's SMAPE: 0.374342\n",
      "6 FOLD SMAPE :  2.6684223674553484\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00269895\tvalid_0's SMAPE: 0.430509\n",
      "[6000]\tvalid_0's l2: 0.00246568\tvalid_0's SMAPE: 0.403797\n",
      "[9000]\tvalid_0's l2: 0.00237707\tvalid_0's SMAPE: 0.392356\n",
      "[12000]\tvalid_0's l2: 0.00233169\tvalid_0's SMAPE: 0.386284\n",
      "[15000]\tvalid_0's l2: 0.00230258\tvalid_0's SMAPE: 0.382387\n",
      "[18000]\tvalid_0's l2: 0.00228621\tvalid_0's SMAPE: 0.380003\n",
      "[21000]\tvalid_0's l2: 0.00227656\tvalid_0's SMAPE: 0.37838\n",
      "[24000]\tvalid_0's l2: 0.00226864\tvalid_0's SMAPE: 0.377171\n",
      "[27000]\tvalid_0's l2: 0.00226308\tvalid_0's SMAPE: 0.376342\n",
      "[30000]\tvalid_0's l2: 0.00225958\tvalid_0's SMAPE: 0.375765\n",
      "[33000]\tvalid_0's l2: 0.0022575\tvalid_0's SMAPE: 0.375369\n",
      "[36000]\tvalid_0's l2: 0.00225549\tvalid_0's SMAPE: 0.375048\n",
      "[39000]\tvalid_0's l2: 0.00225412\tvalid_0's SMAPE: 0.374815\n",
      "[42000]\tvalid_0's l2: 0.00225326\tvalid_0's SMAPE: 0.374657\n",
      "[45000]\tvalid_0's l2: 0.00225251\tvalid_0's SMAPE: 0.374522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48000]\tvalid_0's l2: 0.00225203\tvalid_0's SMAPE: 0.374431\n",
      "7 FOLD SMAPE :  2.6592661509561846\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.0025633\tvalid_0's SMAPE: 0.430069\n",
      "[6000]\tvalid_0's l2: 0.00231512\tvalid_0's SMAPE: 0.402401\n",
      "[9000]\tvalid_0's l2: 0.00222096\tvalid_0's SMAPE: 0.390691\n",
      "[12000]\tvalid_0's l2: 0.00217208\tvalid_0's SMAPE: 0.384267\n",
      "[15000]\tvalid_0's l2: 0.00214815\tvalid_0's SMAPE: 0.380763\n",
      "[18000]\tvalid_0's l2: 0.00213184\tvalid_0's SMAPE: 0.378379\n",
      "[21000]\tvalid_0's l2: 0.0021215\tvalid_0's SMAPE: 0.376825\n",
      "[24000]\tvalid_0's l2: 0.00211488\tvalid_0's SMAPE: 0.37575\n",
      "[27000]\tvalid_0's l2: 0.00211015\tvalid_0's SMAPE: 0.375002\n",
      "[30000]\tvalid_0's l2: 0.00210656\tvalid_0's SMAPE: 0.374455\n",
      "[33000]\tvalid_0's l2: 0.00210434\tvalid_0's SMAPE: 0.37408\n",
      "[36000]\tvalid_0's l2: 0.00210271\tvalid_0's SMAPE: 0.373782\n",
      "[39000]\tvalid_0's l2: 0.0021015\tvalid_0's SMAPE: 0.373586\n",
      "[42000]\tvalid_0's l2: 0.00210069\tvalid_0's SMAPE: 0.373444\n",
      "[45000]\tvalid_0's l2: 0.00209991\tvalid_0's SMAPE: 0.373315\n",
      "[48000]\tvalid_0's l2: 0.00209931\tvalid_0's SMAPE: 0.373212\n",
      "[51000]\tvalid_0's l2: 0.00209876\tvalid_0's SMAPE: 0.373128\n",
      "[54000]\tvalid_0's l2: 0.00209839\tvalid_0's SMAPE: 0.373071\n",
      "[57000]\tvalid_0's l2: 0.00209811\tvalid_0's SMAPE: 0.373023\n",
      "[60000]\tvalid_0's l2: 0.00209785\tvalid_0's SMAPE: 0.372983\n",
      "[63000]\tvalid_0's l2: 0.00209768\tvalid_0's SMAPE: 0.372951\n",
      "[66000]\tvalid_0's l2: 0.00209756\tvalid_0's SMAPE: 0.372923\n",
      "[69000]\tvalid_0's l2: 0.00209742\tvalid_0's SMAPE: 0.372895\n",
      "8 FOLD SMAPE :  2.655202866406829\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00268152\tvalid_0's SMAPE: 0.434969\n",
      "[6000]\tvalid_0's l2: 0.00243372\tvalid_0's SMAPE: 0.407647\n",
      "[9000]\tvalid_0's l2: 0.00233461\tvalid_0's SMAPE: 0.395554\n",
      "[12000]\tvalid_0's l2: 0.00228615\tvalid_0's SMAPE: 0.389473\n",
      "[15000]\tvalid_0's l2: 0.00225786\tvalid_0's SMAPE: 0.386038\n",
      "[18000]\tvalid_0's l2: 0.00224029\tvalid_0's SMAPE: 0.383805\n",
      "[21000]\tvalid_0's l2: 0.00222792\tvalid_0's SMAPE: 0.382162\n",
      "[24000]\tvalid_0's l2: 0.00222051\tvalid_0's SMAPE: 0.381129\n",
      "[27000]\tvalid_0's l2: 0.0022167\tvalid_0's SMAPE: 0.380451\n",
      "[30000]\tvalid_0's l2: 0.00221331\tvalid_0's SMAPE: 0.379942\n",
      "[33000]\tvalid_0's l2: 0.00221124\tvalid_0's SMAPE: 0.379604\n",
      "[36000]\tvalid_0's l2: 0.00220945\tvalid_0's SMAPE: 0.379335\n",
      "[39000]\tvalid_0's l2: 0.00220797\tvalid_0's SMAPE: 0.379121\n",
      "[42000]\tvalid_0's l2: 0.00220683\tvalid_0's SMAPE: 0.378963\n",
      "[45000]\tvalid_0's l2: 0.00220605\tvalid_0's SMAPE: 0.378835\n",
      "[48000]\tvalid_0's l2: 0.00220543\tvalid_0's SMAPE: 0.378737\n",
      "[51000]\tvalid_0's l2: 0.00220496\tvalid_0's SMAPE: 0.378661\n",
      "[54000]\tvalid_0's l2: 0.00220455\tvalid_0's SMAPE: 0.378596\n",
      "[57000]\tvalid_0's l2: 0.00220423\tvalid_0's SMAPE: 0.378552\n",
      "[60000]\tvalid_0's l2: 0.002204\tvalid_0's SMAPE: 0.378522\n",
      "[63000]\tvalid_0's l2: 0.00220379\tvalid_0's SMAPE: 0.378486\n",
      "[66000]\tvalid_0's l2: 0.00220362\tvalid_0's SMAPE: 0.378461\n",
      "[69000]\tvalid_0's l2: 0.0022035\tvalid_0's SMAPE: 0.378443\n",
      "9 FOLD SMAPE :  2.685377751370926\n",
      "==================================================\n",
      "[3000]\tvalid_0's l2: 0.00259278\tvalid_0's SMAPE: 0.424436\n",
      "[6000]\tvalid_0's l2: 0.00236047\tvalid_0's SMAPE: 0.397732\n",
      "[9000]\tvalid_0's l2: 0.00226876\tvalid_0's SMAPE: 0.385769\n",
      "[12000]\tvalid_0's l2: 0.00222547\tvalid_0's SMAPE: 0.379835\n",
      "[15000]\tvalid_0's l2: 0.00219944\tvalid_0's SMAPE: 0.376114\n",
      "[18000]\tvalid_0's l2: 0.00218442\tvalid_0's SMAPE: 0.373812\n",
      "[21000]\tvalid_0's l2: 0.00217392\tvalid_0's SMAPE: 0.372211\n",
      "[24000]\tvalid_0's l2: 0.00216793\tvalid_0's SMAPE: 0.371301\n",
      "[27000]\tvalid_0's l2: 0.00216343\tvalid_0's SMAPE: 0.370612\n",
      "[30000]\tvalid_0's l2: 0.0021599\tvalid_0's SMAPE: 0.370083\n",
      "[33000]\tvalid_0's l2: 0.00215763\tvalid_0's SMAPE: 0.36972\n",
      "[36000]\tvalid_0's l2: 0.00215591\tvalid_0's SMAPE: 0.369437\n",
      "[39000]\tvalid_0's l2: 0.00215473\tvalid_0's SMAPE: 0.369244\n",
      "[42000]\tvalid_0's l2: 0.00215403\tvalid_0's SMAPE: 0.369121\n",
      "[45000]\tvalid_0's l2: 0.00215333\tvalid_0's SMAPE: 0.369003\n",
      "[48000]\tvalid_0's l2: 0.00215291\tvalid_0's SMAPE: 0.368916\n",
      "[51000]\tvalid_0's l2: 0.00215259\tvalid_0's SMAPE: 0.368852\n",
      "[54000]\tvalid_0's l2: 0.00215229\tvalid_0's SMAPE: 0.368794\n",
      "[57000]\tvalid_0's l2: 0.00215206\tvalid_0's SMAPE: 0.368748\n",
      "[60000]\tvalid_0's l2: 0.00215189\tvalid_0's SMAPE: 0.368709\n",
      "[63000]\tvalid_0's l2: 0.00215177\tvalid_0's SMAPE: 0.368684\n",
      "10 FOLD SMAPE :  2.627457247008556\n"
     ]
    }
   ],
   "source": [
    "models_l2 = []\n",
    "\n",
    "n_split_list = [10]\n",
    "smape_l2_score = []\n",
    "\n",
    "for split in n_split_list:\n",
    "    cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=42)\n",
    "    fold = 1\n",
    "    for tri, vai in cv.split(x_train, x_train['build_num']):\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        model = LGBMRegressor(boosting_type='gbdt',\n",
    "                            objective='regression_l2', \n",
    "                            n_estimators=iterations,\n",
    "                            max_depth=12,\n",
    "                            learning_rate=0.01,\n",
    "                            colsample_bytree=0.95,\n",
    "                            subsample=1.0,\n",
    "                            num_leaves=256,\n",
    "                            reg_alpha=0.01,\n",
    "                            reg_lambda=0.01,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)     \n",
    "        model.fit(x_train.iloc[tri], np.log1p(y_train[tri]), \n",
    "                eval_set=[(x_train.iloc[vai], np.log1p(y_train[vai]))], \n",
    "                eval_metric = lgbm_smape,\n",
    "                early_stopping_rounds=patience,\n",
    "                verbose = 3000,\n",
    "                categorical_feature=cat_features)\n",
    "        \n",
    "        pred = model.predict(x_train.iloc[vai])\n",
    "        pred = np.expm1(pred)\n",
    "        score = SMAPE(y_train[vai],pred)\n",
    "        print(f\"{fold} FOLD SMAPE : \",score)\n",
    "        smape_l2_score.append(score)\n",
    "        models_l2.append(model)\n",
    "        \n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0a80361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2065: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:2068: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['build_num', 'holiday', 'hour', 'month', 'weekday']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "C:\\Users\\sangw\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_0's l1: 0.0344541\tvalid_0's SMAPE: 0.483385\n",
      "[6000]\tvalid_0's l1: 0.0329947\tvalid_0's SMAPE: 0.463189\n",
      "[9000]\tvalid_0's l1: 0.0322274\tvalid_0's SMAPE: 0.45272\n",
      "[12000]\tvalid_0's l1: 0.031784\tvalid_0's SMAPE: 0.446613\n",
      "[15000]\tvalid_0's l1: 0.0315057\tvalid_0's SMAPE: 0.442775\n",
      "[18000]\tvalid_0's l1: 0.0312433\tvalid_0's SMAPE: 0.439243\n",
      "[21000]\tvalid_0's l1: 0.03104\tvalid_0's SMAPE: 0.436423\n",
      "[24000]\tvalid_0's l1: 0.0308475\tvalid_0's SMAPE: 0.433755\n",
      "[27000]\tvalid_0's l1: 0.0306317\tvalid_0's SMAPE: 0.430793\n",
      "[30000]\tvalid_0's l1: 0.0304455\tvalid_0's SMAPE: 0.428227\n",
      "[33000]\tvalid_0's l1: 0.0303197\tvalid_0's SMAPE: 0.42648\n",
      "[36000]\tvalid_0's l1: 0.030218\tvalid_0's SMAPE: 0.424967\n",
      "[39000]\tvalid_0's l1: 0.0300706\tvalid_0's SMAPE: 0.422847\n",
      "[42000]\tvalid_0's l1: 0.0299777\tvalid_0's SMAPE: 0.421565\n",
      "[45000]\tvalid_0's l1: 0.029848\tvalid_0's SMAPE: 0.419753\n",
      "[48000]\tvalid_0's l1: 0.0297707\tvalid_0's SMAPE: 0.418666\n",
      "[51000]\tvalid_0's l1: 0.0296906\tvalid_0's SMAPE: 0.417553\n",
      "[54000]\tvalid_0's l1: 0.0296182\tvalid_0's SMAPE: 0.416537\n",
      "[57000]\tvalid_0's l1: 0.0295546\tvalid_0's SMAPE: 0.4156\n",
      "[60000]\tvalid_0's l1: 0.0295006\tvalid_0's SMAPE: 0.414851\n",
      "[63000]\tvalid_0's l1: 0.0294618\tvalid_0's SMAPE: 0.414315\n",
      "[66000]\tvalid_0's l1: 0.0294187\tvalid_0's SMAPE: 0.413717\n",
      "[69000]\tvalid_0's l1: 0.0293756\tvalid_0's SMAPE: 0.413094\n",
      "[72000]\tvalid_0's l1: 0.0293412\tvalid_0's SMAPE: 0.412616\n",
      "[75000]\tvalid_0's l1: 0.0292866\tvalid_0's SMAPE: 0.411872\n",
      "[78000]\tvalid_0's l1: 0.0292449\tvalid_0's SMAPE: 0.411289\n",
      "1 FOLD SMAPE :  2.920966111352012\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0351157\tvalid_0's SMAPE: 0.491823\n",
      "[6000]\tvalid_0's l1: 0.0336437\tvalid_0's SMAPE: 0.471379\n",
      "[9000]\tvalid_0's l1: 0.0329196\tvalid_0's SMAPE: 0.461296\n",
      "[12000]\tvalid_0's l1: 0.0324383\tvalid_0's SMAPE: 0.454478\n",
      "[15000]\tvalid_0's l1: 0.0321314\tvalid_0's SMAPE: 0.450205\n",
      "[18000]\tvalid_0's l1: 0.0318653\tvalid_0's SMAPE: 0.44654\n",
      "[21000]\tvalid_0's l1: 0.0315808\tvalid_0's SMAPE: 0.442592\n",
      "[24000]\tvalid_0's l1: 0.0313705\tvalid_0's SMAPE: 0.439684\n",
      "[27000]\tvalid_0's l1: 0.0311836\tvalid_0's SMAPE: 0.437078\n",
      "[30000]\tvalid_0's l1: 0.0310079\tvalid_0's SMAPE: 0.434563\n",
      "[33000]\tvalid_0's l1: 0.0308761\tvalid_0's SMAPE: 0.432751\n",
      "[36000]\tvalid_0's l1: 0.0307349\tvalid_0's SMAPE: 0.43068\n",
      "[39000]\tvalid_0's l1: 0.0306428\tvalid_0's SMAPE: 0.429424\n",
      "[42000]\tvalid_0's l1: 0.0305431\tvalid_0's SMAPE: 0.427963\n",
      "[45000]\tvalid_0's l1: 0.0304649\tvalid_0's SMAPE: 0.426873\n",
      "[48000]\tvalid_0's l1: 0.0303941\tvalid_0's SMAPE: 0.425846\n",
      "[51000]\tvalid_0's l1: 0.030302\tvalid_0's SMAPE: 0.424541\n",
      "[54000]\tvalid_0's l1: 0.0302536\tvalid_0's SMAPE: 0.423866\n",
      "[57000]\tvalid_0's l1: 0.0301925\tvalid_0's SMAPE: 0.423021\n",
      "2 FOLD SMAPE :  3.0170857127047004\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0344666\tvalid_0's SMAPE: 0.484205\n",
      "[6000]\tvalid_0's l1: 0.0332455\tvalid_0's SMAPE: 0.466957\n",
      "[9000]\tvalid_0's l1: 0.0324138\tvalid_0's SMAPE: 0.455344\n",
      "[12000]\tvalid_0's l1: 0.0318838\tvalid_0's SMAPE: 0.447774\n",
      "[15000]\tvalid_0's l1: 0.031472\tvalid_0's SMAPE: 0.442005\n",
      "[18000]\tvalid_0's l1: 0.0312487\tvalid_0's SMAPE: 0.438965\n",
      "[21000]\tvalid_0's l1: 0.0309996\tvalid_0's SMAPE: 0.435494\n",
      "[24000]\tvalid_0's l1: 0.0307675\tvalid_0's SMAPE: 0.432272\n",
      "[27000]\tvalid_0's l1: 0.0306139\tvalid_0's SMAPE: 0.430123\n",
      "[30000]\tvalid_0's l1: 0.0304054\tvalid_0's SMAPE: 0.427241\n",
      "[33000]\tvalid_0's l1: 0.0303247\tvalid_0's SMAPE: 0.426129\n",
      "[36000]\tvalid_0's l1: 0.0301542\tvalid_0's SMAPE: 0.423773\n",
      "[39000]\tvalid_0's l1: 0.0300046\tvalid_0's SMAPE: 0.421645\n",
      "[42000]\tvalid_0's l1: 0.0299142\tvalid_0's SMAPE: 0.42041\n",
      "[45000]\tvalid_0's l1: 0.02983\tvalid_0's SMAPE: 0.419235\n",
      "[48000]\tvalid_0's l1: 0.0297505\tvalid_0's SMAPE: 0.418048\n",
      "[51000]\tvalid_0's l1: 0.0296272\tvalid_0's SMAPE: 0.416318\n",
      "[54000]\tvalid_0's l1: 0.0295571\tvalid_0's SMAPE: 0.415352\n",
      "[57000]\tvalid_0's l1: 0.0294999\tvalid_0's SMAPE: 0.414548\n",
      "[60000]\tvalid_0's l1: 0.029451\tvalid_0's SMAPE: 0.413896\n",
      "[63000]\tvalid_0's l1: 0.0293667\tvalid_0's SMAPE: 0.41276\n",
      "[66000]\tvalid_0's l1: 0.0293304\tvalid_0's SMAPE: 0.412229\n",
      "[69000]\tvalid_0's l1: 0.0293049\tvalid_0's SMAPE: 0.411861\n",
      "[72000]\tvalid_0's l1: 0.0292852\tvalid_0's SMAPE: 0.411587\n",
      "[75000]\tvalid_0's l1: 0.0292257\tvalid_0's SMAPE: 0.410758\n",
      "[78000]\tvalid_0's l1: 0.0292101\tvalid_0's SMAPE: 0.41053\n",
      "3 FOLD SMAPE :  2.9165062193210702\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0341871\tvalid_0's SMAPE: 0.481434\n",
      "[6000]\tvalid_0's l1: 0.0327455\tvalid_0's SMAPE: 0.461342\n",
      "[9000]\tvalid_0's l1: 0.0320603\tvalid_0's SMAPE: 0.452003\n",
      "[12000]\tvalid_0's l1: 0.0315892\tvalid_0's SMAPE: 0.445541\n",
      "[15000]\tvalid_0's l1: 0.0312239\tvalid_0's SMAPE: 0.440346\n",
      "[18000]\tvalid_0's l1: 0.0309476\tvalid_0's SMAPE: 0.436437\n",
      "[21000]\tvalid_0's l1: 0.0306801\tvalid_0's SMAPE: 0.432639\n",
      "[24000]\tvalid_0's l1: 0.0304534\tvalid_0's SMAPE: 0.429469\n",
      "[27000]\tvalid_0's l1: 0.0302017\tvalid_0's SMAPE: 0.425942\n",
      "[30000]\tvalid_0's l1: 0.0300449\tvalid_0's SMAPE: 0.423689\n",
      "[33000]\tvalid_0's l1: 0.0299229\tvalid_0's SMAPE: 0.421921\n",
      "[36000]\tvalid_0's l1: 0.0298066\tvalid_0's SMAPE: 0.420331\n",
      "[39000]\tvalid_0's l1: 0.0297117\tvalid_0's SMAPE: 0.419012\n",
      "[42000]\tvalid_0's l1: 0.0296286\tvalid_0's SMAPE: 0.417821\n",
      "[45000]\tvalid_0's l1: 0.0295134\tvalid_0's SMAPE: 0.416218\n",
      "[48000]\tvalid_0's l1: 0.0294254\tvalid_0's SMAPE: 0.414776\n",
      "[51000]\tvalid_0's l1: 0.0293746\tvalid_0's SMAPE: 0.414045\n",
      "[54000]\tvalid_0's l1: 0.0293199\tvalid_0's SMAPE: 0.413249\n",
      "[57000]\tvalid_0's l1: 0.0292439\tvalid_0's SMAPE: 0.412165\n",
      "[60000]\tvalid_0's l1: 0.0291855\tvalid_0's SMAPE: 0.411262\n",
      "[63000]\tvalid_0's l1: 0.029141\tvalid_0's SMAPE: 0.410496\n",
      "[66000]\tvalid_0's l1: 0.0291162\tvalid_0's SMAPE: 0.4101\n",
      "[69000]\tvalid_0's l1: 0.0290777\tvalid_0's SMAPE: 0.409552\n",
      "[72000]\tvalid_0's l1: 0.0290291\tvalid_0's SMAPE: 0.408823\n",
      "[75000]\tvalid_0's l1: 0.0289999\tvalid_0's SMAPE: 0.408369\n",
      "[78000]\tvalid_0's l1: 0.0289732\tvalid_0's SMAPE: 0.408009\n",
      "4 FOLD SMAPE :  2.8936412159009492\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.034711\tvalid_0's SMAPE: 0.48836\n",
      "[6000]\tvalid_0's l1: 0.0336096\tvalid_0's SMAPE: 0.473048\n",
      "[9000]\tvalid_0's l1: 0.0328366\tvalid_0's SMAPE: 0.462309\n",
      "[12000]\tvalid_0's l1: 0.0322808\tvalid_0's SMAPE: 0.454654\n",
      "[15000]\tvalid_0's l1: 0.0318228\tvalid_0's SMAPE: 0.447932\n",
      "[18000]\tvalid_0's l1: 0.0315002\tvalid_0's SMAPE: 0.443245\n",
      "[21000]\tvalid_0's l1: 0.0312507\tvalid_0's SMAPE: 0.439796\n",
      "[24000]\tvalid_0's l1: 0.0310883\tvalid_0's SMAPE: 0.437517\n",
      "[27000]\tvalid_0's l1: 0.0308958\tvalid_0's SMAPE: 0.434833\n",
      "[30000]\tvalid_0's l1: 0.0306672\tvalid_0's SMAPE: 0.431837\n",
      "[33000]\tvalid_0's l1: 0.0305303\tvalid_0's SMAPE: 0.429892\n",
      "[36000]\tvalid_0's l1: 0.0304081\tvalid_0's SMAPE: 0.428204\n",
      "[39000]\tvalid_0's l1: 0.030324\tvalid_0's SMAPE: 0.427024\n",
      "[42000]\tvalid_0's l1: 0.0302166\tvalid_0's SMAPE: 0.425499\n",
      "[45000]\tvalid_0's l1: 0.0301003\tvalid_0's SMAPE: 0.423882\n",
      "[48000]\tvalid_0's l1: 0.0300616\tvalid_0's SMAPE: 0.423342\n",
      "[51000]\tvalid_0's l1: 0.0300308\tvalid_0's SMAPE: 0.422903\n",
      "[54000]\tvalid_0's l1: 0.0299448\tvalid_0's SMAPE: 0.421754\n",
      "[57000]\tvalid_0's l1: 0.0299131\tvalid_0's SMAPE: 0.421298\n",
      "[60000]\tvalid_0's l1: 0.0298598\tvalid_0's SMAPE: 0.420519\n",
      "[63000]\tvalid_0's l1: 0.0298113\tvalid_0's SMAPE: 0.419839\n",
      "[66000]\tvalid_0's l1: 0.0297624\tvalid_0's SMAPE: 0.419148\n",
      "[69000]\tvalid_0's l1: 0.0297204\tvalid_0's SMAPE: 0.418576\n",
      "[72000]\tvalid_0's l1: 0.0296803\tvalid_0's SMAPE: 0.418002\n",
      "[75000]\tvalid_0's l1: 0.0296461\tvalid_0's SMAPE: 0.417524\n",
      "[78000]\tvalid_0's l1: 0.0296219\tvalid_0's SMAPE: 0.417183\n",
      "5 FOLD SMAPE :  2.958871732101528\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0344026\tvalid_0's SMAPE: 0.483888\n",
      "[6000]\tvalid_0's l1: 0.0331178\tvalid_0's SMAPE: 0.46589\n",
      "[9000]\tvalid_0's l1: 0.0322687\tvalid_0's SMAPE: 0.45403\n",
      "[12000]\tvalid_0's l1: 0.0318344\tvalid_0's SMAPE: 0.447746\n",
      "[15000]\tvalid_0's l1: 0.0315303\tvalid_0's SMAPE: 0.443328\n",
      "[18000]\tvalid_0's l1: 0.0312161\tvalid_0's SMAPE: 0.438864\n",
      "[21000]\tvalid_0's l1: 0.0310376\tvalid_0's SMAPE: 0.436392\n",
      "[24000]\tvalid_0's l1: 0.0307271\tvalid_0's SMAPE: 0.431962\n",
      "[27000]\tvalid_0's l1: 0.0305339\tvalid_0's SMAPE: 0.429089\n",
      "[30000]\tvalid_0's l1: 0.0303498\tvalid_0's SMAPE: 0.426418\n",
      "[33000]\tvalid_0's l1: 0.0302222\tvalid_0's SMAPE: 0.424669\n",
      "[36000]\tvalid_0's l1: 0.0300764\tvalid_0's SMAPE: 0.422568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39000]\tvalid_0's l1: 0.0299645\tvalid_0's SMAPE: 0.421023\n",
      "[42000]\tvalid_0's l1: 0.0298535\tvalid_0's SMAPE: 0.419489\n",
      "[45000]\tvalid_0's l1: 0.0297558\tvalid_0's SMAPE: 0.418112\n",
      "[48000]\tvalid_0's l1: 0.029678\tvalid_0's SMAPE: 0.416972\n",
      "[51000]\tvalid_0's l1: 0.0295714\tvalid_0's SMAPE: 0.415356\n",
      "[54000]\tvalid_0's l1: 0.0295103\tvalid_0's SMAPE: 0.414274\n",
      "[57000]\tvalid_0's l1: 0.0294707\tvalid_0's SMAPE: 0.413704\n",
      "[60000]\tvalid_0's l1: 0.0293831\tvalid_0's SMAPE: 0.412543\n",
      "[63000]\tvalid_0's l1: 0.0293443\tvalid_0's SMAPE: 0.411892\n",
      "[66000]\tvalid_0's l1: 0.0293051\tvalid_0's SMAPE: 0.411286\n",
      "[69000]\tvalid_0's l1: 0.0292613\tvalid_0's SMAPE: 0.410684\n",
      "[72000]\tvalid_0's l1: 0.0292241\tvalid_0's SMAPE: 0.410192\n",
      "[75000]\tvalid_0's l1: 0.02919\tvalid_0's SMAPE: 0.409702\n",
      "[78000]\tvalid_0's l1: 0.0291718\tvalid_0's SMAPE: 0.40946\n",
      "6 FOLD SMAPE :  2.914982274724576\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0343262\tvalid_0's SMAPE: 0.48357\n",
      "[6000]\tvalid_0's l1: 0.0328812\tvalid_0's SMAPE: 0.463562\n",
      "[9000]\tvalid_0's l1: 0.0321296\tvalid_0's SMAPE: 0.453024\n",
      "[12000]\tvalid_0's l1: 0.0316119\tvalid_0's SMAPE: 0.445732\n",
      "[15000]\tvalid_0's l1: 0.0313226\tvalid_0's SMAPE: 0.441654\n",
      "[18000]\tvalid_0's l1: 0.0310962\tvalid_0's SMAPE: 0.438454\n",
      "[21000]\tvalid_0's l1: 0.0308811\tvalid_0's SMAPE: 0.435345\n",
      "[24000]\tvalid_0's l1: 0.0306619\tvalid_0's SMAPE: 0.432057\n",
      "[27000]\tvalid_0's l1: 0.030519\tvalid_0's SMAPE: 0.430067\n",
      "[30000]\tvalid_0's l1: 0.030357\tvalid_0's SMAPE: 0.427584\n",
      "[33000]\tvalid_0's l1: 0.0302101\tvalid_0's SMAPE: 0.42561\n",
      "[36000]\tvalid_0's l1: 0.030088\tvalid_0's SMAPE: 0.423916\n",
      "[39000]\tvalid_0's l1: 0.0300033\tvalid_0's SMAPE: 0.422739\n",
      "[42000]\tvalid_0's l1: 0.0299141\tvalid_0's SMAPE: 0.421502\n",
      "[45000]\tvalid_0's l1: 0.0298116\tvalid_0's SMAPE: 0.420106\n",
      "[48000]\tvalid_0's l1: 0.0297555\tvalid_0's SMAPE: 0.419334\n",
      "[51000]\tvalid_0's l1: 0.0296898\tvalid_0's SMAPE: 0.418386\n",
      "[54000]\tvalid_0's l1: 0.029616\tvalid_0's SMAPE: 0.41738\n",
      "[57000]\tvalid_0's l1: 0.0295253\tvalid_0's SMAPE: 0.416134\n",
      "[60000]\tvalid_0's l1: 0.0294669\tvalid_0's SMAPE: 0.415322\n",
      "[63000]\tvalid_0's l1: 0.0293971\tvalid_0's SMAPE: 0.414366\n",
      "[66000]\tvalid_0's l1: 0.0293269\tvalid_0's SMAPE: 0.413386\n",
      "[69000]\tvalid_0's l1: 0.0292936\tvalid_0's SMAPE: 0.41291\n",
      "[72000]\tvalid_0's l1: 0.0292409\tvalid_0's SMAPE: 0.412166\n",
      "[75000]\tvalid_0's l1: 0.0291809\tvalid_0's SMAPE: 0.411342\n",
      "[78000]\tvalid_0's l1: 0.0291497\tvalid_0's SMAPE: 0.410913\n",
      "7 FOLD SMAPE :  2.912334901609117\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0346412\tvalid_0's SMAPE: 0.487338\n",
      "[6000]\tvalid_0's l1: 0.0330311\tvalid_0's SMAPE: 0.465123\n",
      "[9000]\tvalid_0's l1: 0.0323157\tvalid_0's SMAPE: 0.455154\n",
      "[12000]\tvalid_0's l1: 0.0319243\tvalid_0's SMAPE: 0.449655\n",
      "[15000]\tvalid_0's l1: 0.0315136\tvalid_0's SMAPE: 0.443971\n",
      "[18000]\tvalid_0's l1: 0.0312711\tvalid_0's SMAPE: 0.440584\n",
      "[21000]\tvalid_0's l1: 0.0310953\tvalid_0's SMAPE: 0.438039\n",
      "[24000]\tvalid_0's l1: 0.030925\tvalid_0's SMAPE: 0.435562\n",
      "[27000]\tvalid_0's l1: 0.0307998\tvalid_0's SMAPE: 0.433743\n",
      "[30000]\tvalid_0's l1: 0.0306166\tvalid_0's SMAPE: 0.430911\n",
      "[33000]\tvalid_0's l1: 0.0304458\tvalid_0's SMAPE: 0.428471\n",
      "[36000]\tvalid_0's l1: 0.0303266\tvalid_0's SMAPE: 0.426813\n",
      "[39000]\tvalid_0's l1: 0.0302407\tvalid_0's SMAPE: 0.425592\n",
      "[42000]\tvalid_0's l1: 0.030104\tvalid_0's SMAPE: 0.423597\n",
      "[45000]\tvalid_0's l1: 0.0300134\tvalid_0's SMAPE: 0.422314\n",
      "[48000]\tvalid_0's l1: 0.0299286\tvalid_0's SMAPE: 0.420975\n",
      "[51000]\tvalid_0's l1: 0.029827\tvalid_0's SMAPE: 0.419502\n",
      "[54000]\tvalid_0's l1: 0.029753\tvalid_0's SMAPE: 0.418438\n",
      "[57000]\tvalid_0's l1: 0.029695\tvalid_0's SMAPE: 0.417606\n",
      "[60000]\tvalid_0's l1: 0.0296494\tvalid_0's SMAPE: 0.416934\n",
      "[63000]\tvalid_0's l1: 0.02961\tvalid_0's SMAPE: 0.416383\n",
      "[66000]\tvalid_0's l1: 0.0295279\tvalid_0's SMAPE: 0.415244\n",
      "[69000]\tvalid_0's l1: 0.0294959\tvalid_0's SMAPE: 0.414823\n",
      "[72000]\tvalid_0's l1: 0.0294709\tvalid_0's SMAPE: 0.41447\n",
      "[75000]\tvalid_0's l1: 0.0294289\tvalid_0's SMAPE: 0.413894\n",
      "[78000]\tvalid_0's l1: 0.0294036\tvalid_0's SMAPE: 0.413536\n",
      "8 FOLD SMAPE :  2.9390870711710244\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0350203\tvalid_0's SMAPE: 0.492175\n",
      "[6000]\tvalid_0's l1: 0.0336576\tvalid_0's SMAPE: 0.473218\n",
      "[9000]\tvalid_0's l1: 0.0328288\tvalid_0's SMAPE: 0.461819\n",
      "[12000]\tvalid_0's l1: 0.0323261\tvalid_0's SMAPE: 0.454887\n",
      "[15000]\tvalid_0's l1: 0.0319326\tvalid_0's SMAPE: 0.449383\n",
      "[18000]\tvalid_0's l1: 0.0317089\tvalid_0's SMAPE: 0.446099\n",
      "[21000]\tvalid_0's l1: 0.0315006\tvalid_0's SMAPE: 0.443127\n",
      "[24000]\tvalid_0's l1: 0.0313266\tvalid_0's SMAPE: 0.440675\n",
      "[27000]\tvalid_0's l1: 0.0311234\tvalid_0's SMAPE: 0.437872\n",
      "[30000]\tvalid_0's l1: 0.0310284\tvalid_0's SMAPE: 0.436602\n",
      "[33000]\tvalid_0's l1: 0.0309219\tvalid_0's SMAPE: 0.435096\n",
      "[36000]\tvalid_0's l1: 0.0307811\tvalid_0's SMAPE: 0.433167\n",
      "[39000]\tvalid_0's l1: 0.0306898\tvalid_0's SMAPE: 0.431898\n",
      "[42000]\tvalid_0's l1: 0.0305863\tvalid_0's SMAPE: 0.430195\n",
      "[45000]\tvalid_0's l1: 0.0304737\tvalid_0's SMAPE: 0.42866\n",
      "[48000]\tvalid_0's l1: 0.0304217\tvalid_0's SMAPE: 0.427944\n",
      "[51000]\tvalid_0's l1: 0.030346\tvalid_0's SMAPE: 0.426864\n",
      "[54000]\tvalid_0's l1: 0.0302995\tvalid_0's SMAPE: 0.426219\n",
      "[57000]\tvalid_0's l1: 0.0302213\tvalid_0's SMAPE: 0.425143\n",
      "[60000]\tvalid_0's l1: 0.0301684\tvalid_0's SMAPE: 0.424413\n",
      "[63000]\tvalid_0's l1: 0.0301199\tvalid_0's SMAPE: 0.423739\n",
      "[66000]\tvalid_0's l1: 0.0300326\tvalid_0's SMAPE: 0.422542\n",
      "[69000]\tvalid_0's l1: 0.0299689\tvalid_0's SMAPE: 0.421667\n",
      "[72000]\tvalid_0's l1: 0.0299438\tvalid_0's SMAPE: 0.42133\n",
      "[75000]\tvalid_0's l1: 0.029909\tvalid_0's SMAPE: 0.420872\n",
      "[78000]\tvalid_0's l1: 0.02988\tvalid_0's SMAPE: 0.420465\n",
      "9 FOLD SMAPE :  2.984079293223312\n",
      "==================================================\n",
      "[3000]\tvalid_0's l1: 0.0342441\tvalid_0's SMAPE: 0.480015\n",
      "[6000]\tvalid_0's l1: 0.0329849\tvalid_0's SMAPE: 0.462441\n",
      "[9000]\tvalid_0's l1: 0.0321744\tvalid_0's SMAPE: 0.451006\n",
      "[12000]\tvalid_0's l1: 0.0317444\tvalid_0's SMAPE: 0.444969\n",
      "[15000]\tvalid_0's l1: 0.0313394\tvalid_0's SMAPE: 0.439359\n",
      "[18000]\tvalid_0's l1: 0.0309708\tvalid_0's SMAPE: 0.434184\n",
      "[21000]\tvalid_0's l1: 0.0307102\tvalid_0's SMAPE: 0.430467\n",
      "[24000]\tvalid_0's l1: 0.0305203\tvalid_0's SMAPE: 0.427759\n",
      "[27000]\tvalid_0's l1: 0.0303609\tvalid_0's SMAPE: 0.425543\n",
      "[30000]\tvalid_0's l1: 0.0301791\tvalid_0's SMAPE: 0.422886\n",
      "[33000]\tvalid_0's l1: 0.0300225\tvalid_0's SMAPE: 0.420691\n",
      "[36000]\tvalid_0's l1: 0.0298193\tvalid_0's SMAPE: 0.417873\n",
      "[39000]\tvalid_0's l1: 0.0297322\tvalid_0's SMAPE: 0.41666\n",
      "[42000]\tvalid_0's l1: 0.0296248\tvalid_0's SMAPE: 0.415189\n",
      "[45000]\tvalid_0's l1: 0.0295486\tvalid_0's SMAPE: 0.414114\n",
      "[48000]\tvalid_0's l1: 0.02946\tvalid_0's SMAPE: 0.412909\n",
      "[51000]\tvalid_0's l1: 0.0293836\tvalid_0's SMAPE: 0.411863\n",
      "[54000]\tvalid_0's l1: 0.0293162\tvalid_0's SMAPE: 0.410863\n",
      "[57000]\tvalid_0's l1: 0.0292239\tvalid_0's SMAPE: 0.409604\n",
      "[60000]\tvalid_0's l1: 0.0291771\tvalid_0's SMAPE: 0.408966\n",
      "[63000]\tvalid_0's l1: 0.0291213\tvalid_0's SMAPE: 0.408234\n",
      "[66000]\tvalid_0's l1: 0.0290816\tvalid_0's SMAPE: 0.407688\n",
      "[69000]\tvalid_0's l1: 0.0290468\tvalid_0's SMAPE: 0.407212\n",
      "[72000]\tvalid_0's l1: 0.0289929\tvalid_0's SMAPE: 0.406477\n",
      "[75000]\tvalid_0's l1: 0.0289421\tvalid_0's SMAPE: 0.405789\n",
      "[78000]\tvalid_0's l1: 0.0288873\tvalid_0's SMAPE: 0.405053\n",
      "10 FOLD SMAPE :  2.8848095658363544\n"
     ]
    }
   ],
   "source": [
    "models_l1 = []\n",
    "\n",
    "n_split_list = [10]\n",
    "smape_l1_score = []\n",
    "\n",
    "for split in n_split_list:\n",
    "    cv = StratifiedKFold(n_splits=split, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold = 1\n",
    "    for tri, vai in cv.split(x_train, x_train['build_num']):\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        model = LGBMRegressor(boosting_type='gbdt',\n",
    "                            objective='regression_l1', \n",
    "                            n_estimators=iterations,\n",
    "                            max_depth=12,\n",
    "                            learning_rate=0.01,\n",
    "                            colsample_bytree=0.95,\n",
    "                            subsample=1.0,\n",
    "                            num_leaves=256,\n",
    "                            reg_alpha=0.01,\n",
    "                            reg_lambda=0.01,\n",
    "                            n_jobs=-1,\n",
    "                            random_state=42)     \n",
    "        model.fit(x_train.iloc[tri], np.log1p(y_train[tri]), \n",
    "                eval_set=[(x_train.iloc[vai], np.log1p(y_train[vai]))], \n",
    "                eval_metric = lgbm_smape,\n",
    "                early_stopping_rounds=patience,\n",
    "                verbose = 3000,\n",
    "                categorical_feature=cat_features)\n",
    "\n",
    "        pred = model.predict(x_train.iloc[vai])\n",
    "        pred = np.expm1(pred)\n",
    "        score = SMAPE(y_train[vai],pred)\n",
    "        print(f\"{fold} FOLD SMAPE : \",score)\n",
    "        smape_l1_score.append(score)\n",
    "        models_l1.append(model)\n",
    "        \n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98af3b9-4d1e-4f68-9b58-a46ecf821ea9",
   "metadata": {},
   "source": [
    "## 4. test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f270545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [16:36<00:00, 99.67s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [26:44<00:00, 160.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds_l2 = []\n",
    "for i in tqdm(range(10)):\n",
    "    pred = models_l2[i].predict(x_test)\n",
    "    pred = np.expm1(pred)\n",
    "    preds_l2.append(pred)\n",
    "    \n",
    "preds_l2 = np.mean(preds_l2 , axis = 0)\n",
    "\n",
    "preds_l1 = []\n",
    "for i in tqdm(range(10)):\n",
    "    pred = models_l1[i].predict(x_test)\n",
    "    pred = np.expm1(pred)\n",
    "    preds_l1.append(pred)\n",
    "    \n",
    "preds_l1 = np.mean(preds_l1 , axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d632229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (preds_l2 + preds_l1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "521eb6cf-f4bd-485f-8f30-3d442c22d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./submission/LGBM_log_ensemble_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0c508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
